{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8159cabd-100f-4427-8529-a859c288b103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "from collate import collate_fn\n",
    "from TdAtt import *\n",
    "from torch import nn\n",
    "import pickle\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4cb856-9b7f-49a4-9ccc-f33e12dea544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4059f5-6533-49ca-94dc-5340dd4f1cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84949da5-f671-44a4-a2df-ef0e4d791b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pkl(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402fc5b3-d569-4a13-82e3-9fe92619d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_wav, str_list = shuffle(get_pkl('mfcc.pkl'), get_pkl('str_list_cut.pkl'), random_state=0)\n",
    "#np_wav = get_pkl('mfcc.pkl')\n",
    "#str_list = get_pkl('str_list_cut.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd6250c-f6cf-47a0-a0b3-da6159b4ba5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainc = len(np_wav) - 500\n",
    "valc = 500"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c35c840e-c529-4365-ab39-cbbc61e47d8d",
   "metadata": {},
   "source": [
    "trainc = 3000\n",
    "valc = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4053f9db-7a63-4272-9491-14b0e5b9ed53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vnp_wav = np_wav[trainc:trainc+valc]\n",
    "vstr_list = str_list[trainc:trainc+valc]\n",
    "np_wav = np_wav[:trainc]\n",
    "str_list = str_list[:trainc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e66648f-8cca-41ab-8698-95e573c0fdee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32230\n",
      "32230\n"
     ]
    }
   ],
   "source": [
    "print(len(np_wav))\n",
    "print(len(str_list))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b8c6d1d-c095-45b3-bdcd-62072b40996e",
   "metadata": {
    "tags": []
   },
   "source": [
    "def pad_tensor(input_tensor, desired_length, fill_value=float('-inf')):\n",
    "    _, input_length, num_channels = input_tensor.size()\n",
    "\n",
    "    #padded_tensor = torch.nn.functional.pad(input_tensor, (0, 0, 0, pad_length))\n",
    "    pad_length = desired_length - input_length\n",
    "    padding = (0, 0, 0, pad_length)  # Pad (left, right, top, bottom)\n",
    "    padded_tensor = torch.nn.functional.pad(input_tensor, padding, value=fill_value)\n",
    "    \n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371b9def-1299-4657-9242-709a8406ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"\", \"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309963d1-a82c-405f-8507-233332e013a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 35\n",
      "[3, 12, 9, 29, 31, 27, 5, 29, 31, 13, 31, 11, 19, 24, 31, 5, 31, 18, 9, 27, 31, 7, 19, 17, 20, 16, 5, 13, 18, 24, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = VectorizeChar(400)\n",
    "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "print(vectorizer(\"hey way i got a new complaint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92fa4481-72a9-485f-88a4-c8c3c8731ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vect_str_list = [vectorizer(txt) for txt in str_list]\n",
    "vvect_str_list = [vectorizer(txt) for txt in vstr_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf89b50-fc99-4511-aeff-5983de30756d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=469):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]`` no\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb91f9da-2f3f-493d-9582-7114ef33d9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpeechFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim, num_2d, num_heads, num_hid=64, layernorm_eps=1e-6):\n",
    "        super(SpeechFeatureEmbedding, self).__init__()\n",
    "        self.num_layers = num_2d\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, num_hid, 3, padding=(0, 1), stride=(2, 1)),\n",
    "                             nn.BatchNorm2d(num_hid),\n",
    "                             nn.LeakyReLU(),\n",
    "                             #nn.Conv2d(num_hid, num_hid, 3, padding=(0, 1), stride=(2, 1)),\n",
    "                             #nn.BatchNorm2d(num_hid),\n",
    "                             #nn.ReLU()\n",
    "                            )\n",
    "        self.tda = nn.ModuleList([TwoD_Attention_layer(in_channels=num_hid, \n",
    "                                                    num_head=num_heads,\n",
    "                                                    emb_dim=embedding_dim,\n",
    "                                                    layernorm_eps=layernorm_eps) for _ in range(self.num_layers)])\n",
    "        self.lin = nn.Linear(embedding_dim * num_hid, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.tda[i](x)\n",
    "        x = self.lin(x.view(x.size(0), x.size(2), -1))\n",
    "        #return torch.squeeze(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca1fad77-5f14-4280-8f40-c274dabf76af",
   "metadata": {},
   "source": [
    "class SpeechFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim, num_hid=64):\n",
    "        super(SpeechFeatureEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, num_hid, 3, stride=(2, 2)),\n",
    "                             nn.BatchNorm2d(num_hid),\n",
    "                             nn.LeakyReLU(),\n",
    "                             nn.Conv2d(num_hid, num_hid, 3, stride=(2, 2)),\n",
    "                             nn.BatchNorm2d(num_hid),\n",
    "                             nn.ReLU()\n",
    "                            )\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(num_hid, 64, 3, padding=\"same\"),\n",
    "                             nn.BatchNorm2d(64),\n",
    "                             nn.LeakyReLU(),\n",
    "                            )\n",
    "        self.lin = nn.Linear(embedding_dim * num_hid, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        #x = self.conv3(x)\n",
    "        x = self.lin(x.view(x.size(0), x.size(2), -1))\n",
    "        #return torch.squeeze(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9775351-7a25-4a88-9e9b-2be1367c1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(embedding_dim, fully_connected_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(fully_connected_dim, embedding_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef8c131-9c36-4d9c-bf11-448f4e304d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self_mha_output, _ = self.mha(inputs, inputs, inputs)\n",
    "        \n",
    "        skip_attention = self.norm1(inputs + self_mha_output)\n",
    "        \n",
    "        ffn_output = self.ffn(skip_attention)\n",
    "        \n",
    "        ffn_output = self.dropout_ffn(ffn_output)\n",
    "        \n",
    "        encoder_layer_out = self.norm2(skip_attention + ffn_output)\n",
    "        \n",
    "        return encoder_layer_out        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a81a01-22da-4d86-9186-b2bf78dcec56",
   "metadata": {},
   "source": [
    "embedding_dim = d_model\n",
    "max_len = ntoken (time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0942e8a-e59f-4508-9178-c2278a237e99",
   "metadata": {},
   "source": [
    "возможно в конце linear слой для настройки ембедінг дім"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d34cf342-592c-472b-9901-31facd964c5c",
   "metadata": {},
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "               max_len, output_dim,  dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(self.embedding_dim, dropout_rate, max_len) # 3\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim[-1])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #x = inputs * math.sqrt(self.embedding_dim) #\n",
    "        x = self.pos_encoding(inputs) #\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = pad_tensor(x, self.output_dim[-2], fill_value=0.)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4a51ac4-1564-4ba7-8311-e8a86ca8f281",
   "metadata": {},
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ad709c-e680-4768-ae9c-6997c0b327a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "               max_len, output_dim,  dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.pad_length = self.output_dim[0] - max_len\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(self.embedding_dim, dropout_rate, max_len) # 3\n",
    "        \n",
    "        \"\"\"self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\"\"\"\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) for _ in range(self.num_layers)])\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, output_dim[-1]),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(max_len, output_dim[-2])\n",
    "            #nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def pad_tensor(self, input_tensor):\n",
    "\n",
    "        padded_tensor = torch.nn.functional.pad(input_tensor, (0, 0, 0, self.pad_length), value=0.)\n",
    "\n",
    "        return padded_tensor        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #x = inputs * math.sqrt(self.embedding_dim) #\n",
    "        x = self.pos_encoding(inputs) #\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        #x = self.linear(x)\n",
    "        #x = self.pad_tensor(x)\n",
    "        x = self.linear2(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c0f4-5a41-4f78-b74a-b91cd54af28e",
   "metadata": {},
   "source": [
    "внутрь декодера (не уровня) maxlen = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfe53555-31e5-4dc3-92a6-368c2b6b1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=35, maxlen=400, embedding_dim=64, dropout_rate=0.1):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.emb = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.pos_emb = PositionalEncoding(embedding_dim, dropout=0, max_len=maxlen) # d_model, dropout=0.1, max_len=469\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.emb(inputs)\n",
    "        #x = x * math.sqrt(self.embedding_dim)\n",
    "        x = self.pos_emb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c13de5c3-0c5a-4705-8560-f771fbe73ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm3 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, inputs, enc_output): # look_ahead_mask !!!!\n",
    "        \n",
    "        seq_len = inputs.size(1)\n",
    "        ahead_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).to(device)\n",
    "        #ahead_mask = self.create_look_ahead_mask2(seq_len)\n",
    "        \n",
    "        self_mha1_output, _ = self.mha1(inputs, inputs, inputs, attn_mask=ahead_mask) # look_ahead_mask !!!! batch\n",
    "        Q1 = self.norm1(self_mha1_output + inputs)\n",
    "        \n",
    "        self_mha2_output, _ = self.mha2(query=Q1, key=enc_output, value=enc_output) # pad mask  ???\n",
    "        skip_attention2 = self.norm2(self_mha2_output + Q1)\n",
    "        \n",
    "        ffn_output = self.ffn(skip_attention2)\n",
    "        drop_output = self.dropout_ffn(ffn_output)\n",
    "        skip3 = self.norm3(drop_output + skip_attention2)\n",
    "        \n",
    "        return skip3\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b29f0448-81f1-43c1-b603-462b31d1eade",
   "metadata": {},
   "source": [
    "def create_look_ahead_mask(self, sequence_length): # + batch size * num heads\n",
    "\n",
    "        mask = torch.tril(torch.ones((sequence_length, sequence_length))).to(device)\n",
    "        return mask\n",
    "    \n",
    "    def create_look_ahead_mask2(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.masked_fill(mask == 0, int(1)).masked_fill(mask == 1, int(0)).bool().to(device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44332f7c-5839-4572-9ec5-957e7908208a",
   "metadata": {},
   "source": [
    "embedding в decoder или в transformer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d4c8b72-4105-42cb-81ad-56c80e25166a",
   "metadata": {},
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,  #target_vocab_size, maximum_position_encoding,\n",
    "                 num_vocab=35, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_emb = TokenEmbedding(num_vocab=num_vocab, maxlen=maxlen, embedding_dim=embedding_dim) # num_vocab=34, maxlen=400, embedding_dim=64\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs, enc_output):\n",
    "        \n",
    "        x = self.token_emb(inputs) # torch.Size([Batch, 400, 64])\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output) # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        return x    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64092044-7045-4940-a447-3973471ce874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,  #target_vocab_size, maximum_position_encoding,\n",
    "                 num_vocab=35, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_emb = TokenEmbedding(num_vocab=num_vocab, maxlen=maxlen, embedding_dim=embedding_dim) # num_vocab=34, maxlen=400, embedding_dim=64\n",
    "        \n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) for _ in range(self.num_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs, enc_output):\n",
    "        \n",
    "        x = self.token_emb(inputs) # torch.Size([Batch, 400, 64])\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output) # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "230fa887-2b9a-4c00-855a-e4d9b8672911",
   "metadata": {},
   "source": [
    "self.layers = nn.ModuleList([\n",
    "            SpeechTransformerDecoderLayer(d_model, num_heads, d_ff, dropout_p, ffnet_style) for _ in range(num_layers)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c06673eb-7748-4abc-9d0d-e10a4b4f9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers_2d, num_layers_encoder, num_layers_decoder, embedding_dim_encoder, embedding_dim_decoder,\n",
    "                 num_heads_2d, num_heads_encoder, num_heads_decoder, fully_connected_dim_encoder, fully_connected_dim_decoder,\n",
    "                 target_vocab_size, max_len_enc, max_len_dec,\n",
    "                 enc_output_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.sfe = SpeechFeatureEmbedding(embedding_dim_encoder, enc_output_dim[-1], num_layers_2d, num_heads_2d, \n",
    "                                          layernorm_eps=layernorm_eps) # torch.Size([1, 116, 20])\n",
    "        \n",
    "        self.encoder = Encoder(num_layers=num_layers_encoder,\n",
    "                               embedding_dim=enc_output_dim[-1],\n",
    "                               #embedding_dim=embedding_dim_encoder,\n",
    "                               num_heads=num_heads_encoder,\n",
    "                               fully_connected_dim=fully_connected_dim_encoder,\n",
    "                               max_len=max_len_enc, # 116\n",
    "                               output_dim=enc_output_dim, # (400, 64)\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps) # torch.Size([1, 400, 64])\n",
    "        \n",
    "        \"\"\"encoder = Encoder(num_layers=2,\n",
    "                            embedding_dim=20,\n",
    "                            num_heads=10,\n",
    "                            fully_connected_dim=100,\n",
    "                            max_len=116,\n",
    "                            output_dim=(400, 64),\n",
    "                            dropout_rate=0)\"\"\"\n",
    "        \n",
    "        self.decoder = Decoder(num_layers=num_layers_decoder, \n",
    "                               embedding_dim=embedding_dim_decoder,\n",
    "                               num_heads=num_heads_decoder,\n",
    "                               fully_connected_dim=fully_connected_dim_decoder,\n",
    "                               num_vocab=target_vocab_size, # num_vocab=35\n",
    "                               maxlen=max_len_dec,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "        \"\"\"num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 num_vocab=34, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6\"\"\"\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.LazyLinear(target_vocab_size),\n",
    "            nn.Linear(embedding_dim_decoder, target_vocab_size),\n",
    "            #nn.Softmax(dim=-1) # 1\n",
    "            )\n",
    "        \n",
    "    def forward(self, input_spect_t, output_vect_str):\n",
    "        \n",
    "        enc_input = self.sfe(input_spect_t) # torch.Size([1, 116, 20]) 1 = N batches\n",
    "        \n",
    "        enc_output = self.encoder(enc_input)\n",
    "        \n",
    "        dec_output = self.decoder(output_vect_str, enc_output)  # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        final_output = self.linear(dec_output)\n",
    "        \n",
    "        return final_output  # [Batch, 400, 35]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e08ffe3-6809-4b4a-8e4f-c246bffe6a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(np_wav, vect_str_list)\n",
    "vdataset = CustomDataset(vnp_wav, vvect_str_list)\n",
    "dataloader = DataLoader(dataset, batch_size=18, collate_fn=collate_fn, num_workers=2) # + num thread num_workers=6,\n",
    "vdataloader = DataLoader(vdataset, batch_size=18, collate_fn=collate_fn, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7851a51d-c392-41b9-b944-887656fb4661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, opt, loader):\n",
    "    loss_per_batches = 0\n",
    "    elapsed = 0\n",
    "    start_epoch2 = time.time()\n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        start_epoch = time.time()\n",
    "        features, labels = data\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        dec_input = labels[:, :-1]\n",
    "        dec_target = labels[:, 1:]\n",
    "        \n",
    "        y_pred = model(features, dec_input)\n",
    "        \n",
    "        #one_hot = nn.functional.one_hot(labels, 35).type(torch.float)\n",
    "        #indices = torch.nonzero(torch.eq(labels, 0))[0].item()\n",
    "        #print(str(labels) + \"y_pred\")\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        loss = loss_fn(y_pred.view(-1, y_pred.size(-1)), dec_target.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        loss_per_batches += loss\n",
    "        \n",
    "        end_epoch = time.time()\n",
    "        elapsed += (end_epoch - start_epoch)\n",
    "        \n",
    "    print(\"train = \" + str(elapsed))\n",
    "    print(\"train + load = \" + str(time.time() - start_epoch2))\n",
    "    return loss_per_batches/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ba98d9f-64f4-4d36-a1e0-77f91476f53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, opt, train_loader, val_loader, save_treshold=5, epochs=10, model_name='model_name'):\n",
    "        \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/' + model_name + '_{}'.format(timestamp))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = train_step(model, loss_fn, opt, train_loader)\n",
    "        model.eval()\n",
    "        \n",
    "        vloss = 0\n",
    "        counter = 0\n",
    "        with torch.inference_mode():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vfeatures, vlabels = vdata\n",
    "                vfeatures, vlabels = vfeatures.to(device), vlabels.to(device)\n",
    "                dec_input = vlabels[:, :-1]\n",
    "                dec_target = vlabels[:, 1:]\n",
    "\n",
    "                y_pred = model(vfeatures, dec_input)\n",
    "\n",
    "                vloss += loss_fn(y_pred.view(-1, y_pred.size(-1)), dec_target.contiguous().view(-1))\n",
    "                counter = i\n",
    "\n",
    "        avg_vloss = vloss / (counter + 1)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        \n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "        \n",
    "        if (epoch + 1) % save_treshold == 0:\n",
    "            model_path = model_name +'_{}_{}'.format(timestamp, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        end_epoch = time.time()\n",
    "        elapsed = end_epoch - start_epoch\n",
    "        print(\"Time per epoch {}s\".format(elapsed))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c093728-3229-40d5-b80b-4511f859a0de",
   "metadata": {},
   "source": [
    "                (self, num_layers_2d, num_layers_encoder, num_layers_decoder, embedding_dim_encoder, embedding_dim_decoder,\n",
    "                 num_heads_2d, num_heads_encoder, num_heads_decoder, fully_connected_dim_encoder, fully_connected_dim_decoder,\n",
    "                 target_vocab_size, max_len_enc, max_len_dec,\n",
    "                 enc_output_dim, dropout_rate=0.1, layernorm_eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45608453-8bfe-48fc-88e5-92329c4d6060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (sfe): SpeechFeatureEmbedding(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 1), padding=(0, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (tda): ModuleList(\n",
       "      (0-1): 2 x TwoD_Attention_layer(\n",
       "        (convq): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (convk): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (convv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (bnq): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bnk): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bnv): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (ln): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "        (final_conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (act1): ReLU()\n",
       "        (final_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (bnf1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bnf2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (lin): Linear(in_features=1280, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout_ffn): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (linear2): Sequential(\n",
       "      (0): Linear(in_features=234, out_features=399, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_emb): TokenEmbedding(\n",
       "      (emb): Embedding(35, 64)\n",
       "      (pos_emb): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dec_layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (mha1): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout_ffn): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=35, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(2, 6, 3, 20, 64, #10, 8, 127, 64 (512)\n",
    "                32, 16, 8, 1024, 1024,\n",
    "                35, 234, 399, # 116\n",
    "                (399, 64),\n",
    "                dropout_rate=0.25)  # (, 64)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1754d2fb-6d7b-4124-bb61-18d3401779f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─SpeechFeatureEmbedding: 1-1                 --\n",
      "|    └─Sequential: 2-1                        --\n",
      "|    |    └─Conv2d: 3-1                       640\n",
      "|    |    └─BatchNorm2d: 3-2                  128\n",
      "|    |    └─LeakyReLU: 3-3                    --\n",
      "|    └─ModuleList: 2-2                        --\n",
      "|    |    └─TwoD_Attention_layer: 3-4         166,664\n",
      "|    |    └─TwoD_Attention_layer: 3-5         166,664\n",
      "|    └─Linear: 2-3                            81,984\n",
      "├─Encoder: 1-2                                --\n",
      "|    └─PositionalEncoding: 2-4                --\n",
      "|    |    └─Dropout: 3-6                      --\n",
      "|    └─ModuleList: 2-5                        --\n",
      "|    |    └─EncoderLayer: 3-7                 149,056\n",
      "|    |    └─EncoderLayer: 3-8                 149,056\n",
      "|    |    └─EncoderLayer: 3-9                 149,056\n",
      "|    |    └─EncoderLayer: 3-10                149,056\n",
      "|    |    └─EncoderLayer: 3-11                149,056\n",
      "|    |    └─EncoderLayer: 3-12                149,056\n",
      "|    └─Sequential: 2-6                        --\n",
      "|    |    └─Linear: 3-13                      4,160\n",
      "|    |    └─ReLU: 3-14                        --\n",
      "|    └─Sequential: 2-7                        --\n",
      "|    |    └─Linear: 3-15                      93,765\n",
      "├─Decoder: 1-3                                --\n",
      "|    └─TokenEmbedding: 2-8                    --\n",
      "|    |    └─Embedding: 3-16                   2,240\n",
      "|    |    └─PositionalEncoding: 3-17          --\n",
      "|    └─ModuleList: 2-9                        --\n",
      "|    |    └─DecoderLayer: 3-18                165,824\n",
      "|    |    └─DecoderLayer: 3-19                165,824\n",
      "|    |    └─DecoderLayer: 3-20                165,824\n",
      "|    └─Dropout: 2-10                          --\n",
      "├─Sequential: 1-4                             --\n",
      "|    └─Linear: 2-11                           2,275\n",
      "======================================================================\n",
      "Total params: 1,910,328\n",
      "Trainable params: 1,910,328\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "summary(model)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c3acadc-0eeb-4a92-ab33-dbe1f12b45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer2(nn.Module):\n",
    "    def __init__(self, target_vocab_size, d_model, nhead, num_layers):\n",
    "        super(Transformer2, self).__init__()\n",
    "\n",
    "        #self.encoder = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.encoder = SpeechFeatureEmbedding(20, d_model) \n",
    "        self.decoder = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout=0.2, max_len=400)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers[0],\n",
    "                                          num_decoder_layers=num_layers[1], dropout=0.2)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.encoder(src)\n",
    "        tgt_emb = self.decoder(tgt)\n",
    "        src_emb = self.pos_enc(src_emb)\n",
    "        tgt_emb = self.pos_enc(tgt_emb)\n",
    "        \n",
    "        src_emb = src_emb.permute(1, 0, 2)  # Change shape from [batch_size, seq_len_src, embedding_dim] to [seq_len_src, batch_size, embedding_dim]\n",
    "        tgt_emb = tgt_emb.permute(1, 0, 2)\n",
    "\n",
    "        memory = self.transformer.encoder(src_emb)\n",
    "\n",
    "        tgt_len = tgt_emb.size(0)\n",
    "        tgt_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool().to(device)\n",
    "\n",
    "        output = self.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "\n",
    "        output = output.permute(1, 0, 2)  # Change shape back to [batch_size, seq_len_tgt, d_model]\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eeb36273-2aed-4d87-bf3d-616733d44135",
   "metadata": {},
   "source": [
    "model2 = Transformer2(35, 64, 8, (6, 3))\n",
    "loss_fn2 = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ceaf449-4d96-4e5b-8763-5e5a9440a7ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "summary(model2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "091f5783-f7a5-4697-a7d8-b3e3079d1506",
   "metadata": {},
   "source": [
    "train(model2, loss_fn2, optimizer2, dataloader, vdataloader, epochs=100, model_name=model2.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3347f3-e9d6-4fc8-ade6-9a49518a0f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "train = 1167.1600978374481\n",
      "train + load = 1187.6655128002167\n",
      "LOSS train 2.246258020401001 valid 1.9474823474884033\n",
      "Time per epoch 1197.1700315475464s\n",
      "EPOCH 2:\n",
      "train = 1145.2021024227142\n",
      "train + load = 1174.6573629379272\n",
      "LOSS train 1.9521785974502563 valid 1.7777577638626099\n",
      "Time per epoch 1185.8111011981964s\n",
      "EPOCH 3:\n",
      "train = 1145.2888309955597\n",
      "train + load = 1176.4415941238403\n",
      "LOSS train 1.8457094430923462 valid 1.7042498588562012\n",
      "Time per epoch 1191.8459913730621s\n",
      "EPOCH 4:\n",
      "train = 1139.344937324524\n",
      "train + load = 1181.2651300430298\n",
      "LOSS train 1.791157603263855 valid 1.6716454029083252\n",
      "Time per epoch 1192.7714574337006s\n",
      "EPOCH 5:\n",
      "train = 1139.1916108131409\n",
      "train + load = 1185.990293264389\n",
      "LOSS train 1.7564611434936523 valid 1.650112509727478\n",
      "Time per epoch 1197.6828706264496s\n",
      "EPOCH 6:\n",
      "train = 1139.3652882575989\n",
      "train + load = 1188.5825641155243\n",
      "LOSS train 1.7313385009765625 valid 1.6415069103240967\n",
      "Time per epoch 1199.7153825759888s\n",
      "EPOCH 7:\n",
      "train = 1139.4961307048798\n",
      "train + load = 1193.5214304924011\n",
      "LOSS train 1.7107232809066772 valid 1.6301432847976685\n",
      "Time per epoch 1204.4262511730194s\n",
      "EPOCH 8:\n",
      "train = 1139.6004366874695\n",
      "train + load = 1196.097806930542\n",
      "LOSS train 1.6960030794143677 valid 1.6186985969543457\n",
      "Time per epoch 1206.8380093574524s\n",
      "EPOCH 9:\n",
      "train = 1139.3004310131073\n",
      "train + load = 1199.2342472076416\n",
      "LOSS train 1.6820595264434814 valid 1.6301729679107666\n",
      "Time per epoch 1210.2060933113098s\n",
      "EPOCH 10:\n",
      "train = 1139.1495425701141\n",
      "train + load = 1201.4119579792023\n",
      "LOSS train 1.6715171337127686 valid 1.639107346534729\n",
      "Time per epoch 1212.3812291622162s\n",
      "EPOCH 11:\n",
      "train = 1138.9782333374023\n",
      "train + load = 1204.2442944049835\n",
      "LOSS train 1.6619930267333984 valid 1.611796259880066\n",
      "Time per epoch 1215.8249912261963s\n",
      "EPOCH 12:\n",
      "train = 1139.0051953792572\n",
      "train + load = 1204.9782061576843\n",
      "LOSS train 1.6537444591522217 valid 1.6092398166656494\n",
      "Time per epoch 1215.9473052024841s\n",
      "EPOCH 13:\n",
      "train = 1139.5959742069244\n",
      "train + load = 1204.638986825943\n",
      "LOSS train 1.6466444730758667 valid 1.5859578847885132\n",
      "Time per epoch 1215.6891677379608s\n",
      "EPOCH 14:\n",
      "train = 1139.2465283870697\n",
      "train + load = 1207.2462770938873\n",
      "LOSS train 1.6394966840744019 valid 1.5768647193908691\n",
      "Time per epoch 1218.2956891059875s\n",
      "EPOCH 15:\n",
      "train = 1138.475342988968\n",
      "train + load = 1208.3566200733185\n",
      "LOSS train 1.6346147060394287 valid 1.5655711889266968\n",
      "Time per epoch 1219.5041801929474s\n",
      "EPOCH 16:\n",
      "train = 1139.015098810196\n",
      "train + load = 1209.837274312973\n",
      "LOSS train 1.629452109336853 valid 1.5828773975372314\n",
      "Time per epoch 1221.0679020881653s\n",
      "EPOCH 17:\n",
      "train = 1139.1744782924652\n",
      "train + load = 1209.3753635883331\n",
      "LOSS train 1.6239567995071411 valid 1.5599195957183838\n",
      "Time per epoch 1220.5299463272095s\n",
      "EPOCH 18:\n",
      "train = 1138.8007037639618\n",
      "train + load = 1210.2415158748627\n",
      "LOSS train 1.6203124523162842 valid 1.5414913892745972\n",
      "Time per epoch 1223.555861711502s\n",
      "EPOCH 19:\n",
      "train = 1139.48464012146\n",
      "train + load = 1216.9370651245117\n",
      "LOSS train 1.6160677671432495 valid 1.5388712882995605\n",
      "Time per epoch 1228.3523540496826s\n",
      "EPOCH 20:\n",
      "train = 1139.512249469757\n",
      "train + load = 1217.0175807476044\n",
      "LOSS train 1.6123281717300415 valid 1.5414072275161743\n",
      "Time per epoch 1228.8072707653046s\n",
      "EPOCH 21:\n",
      "train = 1139.080005645752\n",
      "train + load = 1214.9148497581482\n",
      "LOSS train 1.608420729637146 valid 1.5188854932785034\n",
      "Time per epoch 1226.8628039360046s\n",
      "EPOCH 22:\n",
      "train = 1138.8744094371796\n",
      "train + load = 1214.1772100925446\n",
      "LOSS train 1.6053277254104614 valid 1.5142236948013306\n",
      "Time per epoch 1225.593576669693s\n",
      "EPOCH 23:\n",
      "train = 1139.3627438545227\n",
      "train + load = 1215.1442170143127\n",
      "LOSS train 1.6029106378555298 valid 1.5052913427352905\n",
      "Time per epoch 1226.6034998893738s\n",
      "EPOCH 24:\n",
      "train = 1139.0260701179504\n",
      "train + load = 1214.5574610233307\n",
      "LOSS train 1.599405288696289 valid 1.504817247390747\n",
      "Time per epoch 1225.839054107666s\n",
      "EPOCH 25:\n",
      "train = 1139.152616739273\n",
      "train + load = 1214.990160703659\n",
      "LOSS train 1.5971087217330933 valid 1.5114960670471191\n",
      "Time per epoch 1228.6933381557465s\n",
      "EPOCH 26:\n",
      "train = 1140.1339633464813\n",
      "train + load = 1220.0775187015533\n",
      "LOSS train 1.5942919254302979 valid 1.497327446937561\n",
      "Time per epoch 1231.9885625839233s\n",
      "EPOCH 27:\n",
      "train = 1140.0685231685638\n",
      "train + load = 1219.9630148410797\n",
      "LOSS train 1.5913615226745605 valid 1.4937129020690918\n",
      "Time per epoch 1232.2048125267029s\n",
      "EPOCH 28:\n",
      "train = 1140.2933127880096\n",
      "train + load = 1220.6518363952637\n",
      "LOSS train 1.5888793468475342 valid 1.4877121448516846\n",
      "Time per epoch 1231.98886013031s\n",
      "EPOCH 29:\n",
      "train = 1140.0463328361511\n",
      "train + load = 1220.416932106018\n",
      "LOSS train 1.585961103439331 valid 1.4890036582946777\n",
      "Time per epoch 1231.8065066337585s\n",
      "EPOCH 30:\n",
      "train = 1140.3791630268097\n",
      "train + load = 1219.3368935585022\n",
      "LOSS train 1.5833247900009155 valid 1.4836474657058716\n",
      "Time per epoch 1231.2135791778564s\n",
      "EPOCH 31:\n",
      "train = 1141.0376029014587\n",
      "train + load = 1222.069828748703\n",
      "LOSS train 1.5810277462005615 valid 1.4858936071395874\n",
      "Time per epoch 1234.1926662921906s\n",
      "EPOCH 32:\n",
      "train = 1140.493557691574\n",
      "train + load = 1225.2539896965027\n",
      "LOSS train 1.5783360004425049 valid 1.4795676469802856\n",
      "Time per epoch 1237.8346886634827s\n",
      "EPOCH 33:\n",
      "train = 1141.8989939689636\n",
      "train + load = 1224.2786929607391\n",
      "LOSS train 1.5763784646987915 valid 1.484279751777649\n",
      "Time per epoch 1237.9295539855957s\n",
      "EPOCH 34:\n",
      "train = 1144.3950853347778\n",
      "train + load = 1237.9643881320953\n",
      "LOSS train 1.5740437507629395 valid 1.4797316789627075\n",
      "Time per epoch 1252.1619007587433s\n",
      "EPOCH 35:\n",
      "train = 1140.8079385757446\n",
      "train + load = 1230.1640455722809\n",
      "LOSS train 1.5719177722930908 valid 1.4730933904647827\n",
      "Time per epoch 1244.1541109085083s\n",
      "EPOCH 36:\n",
      "train = 1141.983768939972\n",
      "train + load = 1225.6130559444427\n",
      "LOSS train 1.5709593296051025 valid 1.477352261543274\n",
      "Time per epoch 1239.6621108055115s\n",
      "EPOCH 37:\n",
      "train = 1143.8064742088318\n",
      "train + load = 1233.1864330768585\n",
      "LOSS train 1.56870698928833 valid 1.4687788486480713\n",
      "Time per epoch 1247.5428218841553s\n",
      "EPOCH 38:\n",
      "train = 1141.4162125587463\n",
      "train + load = 1230.511566877365\n",
      "LOSS train 1.5670255422592163 valid 1.4713976383209229\n",
      "Time per epoch 1245.1730697154999s\n",
      "EPOCH 39:\n",
      "train = 1141.3311738967896\n",
      "train + load = 1229.9581067562103\n",
      "LOSS train 1.5651956796646118 valid 1.4738389253616333\n",
      "Time per epoch 1243.9534938335419s\n",
      "EPOCH 40:\n",
      "train = 1143.0776171684265\n",
      "train + load = 1240.7514266967773\n",
      "LOSS train 1.5646698474884033 valid 1.4741456508636475\n",
      "Time per epoch 1256.0769755840302s\n",
      "EPOCH 41:\n",
      "train = 1142.1399409770966\n",
      "train + load = 1232.3827464580536\n",
      "LOSS train 1.5625593662261963 valid 1.4700185060501099\n",
      "Time per epoch 1247.1172440052032s\n",
      "EPOCH 42:\n",
      "train = 1141.9672927856445\n",
      "train + load = 1233.0677073001862\n",
      "LOSS train 1.5607892274856567 valid 1.4701626300811768\n",
      "Time per epoch 1248.3237655162811s\n",
      "EPOCH 43:\n",
      "train = 1141.2612965106964\n",
      "train + load = 1232.5635216236115\n",
      "LOSS train 1.5593844652175903 valid 1.4665921926498413\n",
      "Time per epoch 1246.015615940094s\n",
      "EPOCH 44:\n",
      "train = 1141.9385673999786\n",
      "train + load = 1230.4801874160767\n",
      "LOSS train 1.5583938360214233 valid 1.4596893787384033\n",
      "Time per epoch 1245.1858749389648s\n",
      "EPOCH 45:\n",
      "train = 1142.5523700714111\n",
      "train + load = 1247.22820186615\n",
      "LOSS train 1.5567913055419922 valid 1.4615885019302368\n",
      "Time per epoch 1263.060832977295s\n",
      "EPOCH 46:\n",
      "train = 1145.41104722023\n",
      "train + load = 1246.9892904758453\n",
      "LOSS train 1.5548408031463623 valid 1.4628840684890747\n",
      "Time per epoch 1261.736082315445s\n",
      "EPOCH 47:\n",
      "train = 1144.1354546546936\n",
      "train + load = 1243.5749204158783\n",
      "LOSS train 1.553798794746399 valid 1.4634588956832886\n",
      "Time per epoch 1258.9439940452576s\n",
      "EPOCH 48:\n",
      "train = 1144.0039472579956\n",
      "train + load = 1251.0116574764252\n",
      "LOSS train 1.5524146556854248 valid 1.4570900201797485\n",
      "Time per epoch 1266.3468098640442s\n",
      "EPOCH 49:\n",
      "train = 1143.535930633545\n",
      "train + load = 1245.9194672107697\n",
      "LOSS train 1.5511492490768433 valid 1.4621235132217407\n",
      "Time per epoch 1260.8014736175537s\n",
      "EPOCH 50:\n",
      "train = 1164.2742030620575\n",
      "train + load = 1264.4197783470154\n",
      "LOSS train 1.550254464149475 valid 1.4630191326141357\n",
      "Time per epoch 1279.7401053905487s\n",
      "EPOCH 51:\n",
      "train = 1142.997627735138\n",
      "train + load = 1243.75772190094\n",
      "LOSS train 1.5486360788345337 valid 1.4594473838806152\n",
      "Time per epoch 1259.0938129425049s\n",
      "EPOCH 52:\n",
      "train = 1143.5341837406158\n",
      "train + load = 1251.704537153244\n",
      "LOSS train 1.547749638557434 valid 1.4544901847839355\n",
      "Time per epoch 1266.3239159584045s\n",
      "EPOCH 53:\n",
      "train = 1144.1342160701752\n",
      "train + load = 1249.2364008426666\n",
      "LOSS train 1.5457152128219604 valid 1.4558682441711426\n",
      "Time per epoch 1264.2447600364685s\n",
      "EPOCH 54:\n",
      "train = 1145.0046181678772\n",
      "train + load = 1251.987104177475\n",
      "LOSS train 1.5455511808395386 valid 1.4574921131134033\n",
      "Time per epoch 1266.6558527946472s\n",
      "EPOCH 55:\n",
      "train = 1159.7878201007843\n",
      "train + load = 1262.0859155654907\n",
      "LOSS train 1.543871521949768 valid 1.456017017364502\n",
      "Time per epoch 1277.381590127945s\n",
      "EPOCH 56:\n",
      "train = 1142.054277896881\n",
      "train + load = 1245.458487033844\n",
      "LOSS train 1.5426390171051025 valid 1.455933928489685\n",
      "Time per epoch 1261.035002708435s\n",
      "EPOCH 57:\n",
      "train = 1143.4648532867432\n",
      "train + load = 1251.6221935749054\n",
      "LOSS train 1.5411858558654785 valid 1.4556019306182861\n",
      "Time per epoch 1266.442207813263s\n",
      "EPOCH 58:\n",
      "train = 1142.9653437137604\n",
      "train + load = 1254.69038772583\n",
      "LOSS train 1.5399984121322632 valid 1.4535306692123413\n",
      "Time per epoch 1269.548987865448s\n",
      "EPOCH 59:\n",
      "train = 1156.9508986473083\n",
      "train + load = 1257.6754035949707\n",
      "LOSS train 1.5391079187393188 valid 1.4491673707962036\n",
      "Time per epoch 1271.6297953128815s\n",
      "EPOCH 60:\n",
      "train = 1146.9163510799408\n",
      "train + load = 1249.7697403430939\n",
      "LOSS train 1.5373703241348267 valid 1.4483411312103271\n",
      "Time per epoch 1264.506064414978s\n",
      "EPOCH 61:\n",
      "train = 1156.091477394104\n",
      "train + load = 1261.7756762504578\n",
      "LOSS train 1.5374122858047485 valid 1.4479156732559204\n",
      "Time per epoch 1277.3213465213776s\n",
      "EPOCH 62:\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    train(model, loss_fn, optimizer, dataloader, vdataloader, epochs=500, model_name=model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bcb02-62f2-44ee-afda-6dded5a3c669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.triu(torch.ones(5, 5, dtype=torch.bool), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971acd6e-64bb-4ee0-9078-8c188bd0f868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1482345-fbad-4fc4-bd23-456e2ac161bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e5fbc-d9bb-4444-b694-5b4512a2e120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data_mfcc(np_wav, nperseg=1024, samplerate=24000):\n",
    "    mfcc = librosa.feature.mfcc(y=np_wav.astype(float), sr=samplerate, hop_length=nperseg)\n",
    "    pd = sklearn.preprocessing.scale(mfcc, axis=1)\n",
    "    new_shape = int(469 * 1024/nperseg)\n",
    "    pad = np.pad(pd, ((0, 0), (0, new_shape - pd.shape[1])), mode='constant')\n",
    "    return torch.tensor(np.expand_dims(np.swapaxes(pad,0,1), axis=0), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d4b8d-3b0e-4780-a7e5-648c0f5ff2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = torch.tensor([3]).to(device)\n",
    "dec_out = list()\n",
    "for i in range(400 - 1):\n",
    "    res = model(torch.tensor(np_wav[0], dtype=torch.float).unsqueeze(0).to(device), \n",
    "            inp.unsqueeze(0).to(device)).squeeze(0)\n",
    "    #print(len(inp))\n",
    "    soft_out = nn.functional.softmax(res, dim=-1)\n",
    "    last_logit = soft_out.argmax(dim=-1)[-1].unsqueeze(0)\n",
    "    #print(last_logit)\n",
    "    dec_out.append(last_logit)\n",
    "    inp = torch.cat((inp, last_logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03fe40-6004-4f6c-8cf4-e06dedd781ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = \"\"\n",
    "for x in inp:\n",
    "    out += voc[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb690a-dc94-448e-a67f-1a10dd27b996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4613d22-e584-4d80-b98a-586845078a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f5f13-f766-44a5-9a76-11115cfe5a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0d992-3c0a-49d1-aa9d-f2ec1d8681dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"0\" * 399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720efd7-9b73-47e7-bde4-bc76e1ab0d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd25f24-df2e-4080-961c-1dfb5356ff99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp2 = \"<\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba293db-64fd-4d61-a1c7-8899a2182476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac7f1b-8897-4b8e-b535-e151cd36fd26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = model(torch.tensor(vnp_wav[15], dtype=torch.float).unsqueeze(0).to(device), \n",
    "            torch.tensor(vvect_str_list[15])[:-1].unsqueeze(0).to(device)).squeeze(0)\n",
    "\n",
    "output_str = str()\n",
    "voc = vectorizer.get_vocabulary()\n",
    "\n",
    "inp = \"<\"\n",
    "for i in range(399):\n",
    "    output_str += voc[res[i].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13171cd-45c2-4ad2-bac0-0a423be6ea87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = model(process_data_mfcc(np_wav[0]).unsqueeze(0).to(device), \n",
    "            torch.tensor(vectorizer(\"<matth\"))[:-1].unsqueeze(0).to(device)).squeeze(0)\n",
    "\n",
    "output_str = str()\n",
    "voc = vectorizer.get_vocabulary()\n",
    "\n",
    "for i in range(399):\n",
    "    output_str += voc[res[i].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1d70b-9412-4a61-94a6-0ef1f8528db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20076383-698c-426c-8804-a077ba0c1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vstr_list[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42253f3d-0c90-4338-aef5-2d9962c0660f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_list[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fe2e9-da00-4d29-af9d-3daa96dd674a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_data_mfcc(np_wav[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02c289-bcba-4043-be53-a154c9881f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = torch.nonzero(torch.eq(tensor, 0))[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48baabcf-e896-43e0-9529-eee03154af5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdcca07-9f07-4cae-8111-d0e04bcc5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.empty(32, 400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226acd3-73ad-465e-a18b-b0e79f649acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tensor.view(-1, tensor.size(-1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc94c67-faf7-4025-898f-bbd4cd605b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf2021-ba53-4d86-b9f0-9b614b0ac369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabd59b-a717-4515-990b-cf2284509ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = process_data_mfcc(np_wav[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72665a-5e82-49e7-81ba-b1aaa5311c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa5f75-e3e3-4836-a529-7835920a8751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(sequence_length): # + batch size * num heads\n",
    "\n",
    "    mask = torch.tril(torch.ones((sequence_length, sequence_length)))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24572524-81c1-447b-b31e-4e55f16ead43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def func(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.masked_fill(mask == 0, int(1)).masked_fill(mask == 1, int(0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d00f4-36e6-4be0-b54a-90c31aeeb637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "func(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d51d1d-0510-4cd6-b0dc-ce1d463b8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_t = nn.MultiheadAttention(embed_dim=2, num_heads=1, dropout=0, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a9ae0-6731-443c-ba40-ca41ae1d115a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[[2, 3],[4, 5],[6, 7]]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e57f7c-490a-4def-9fff-5257da5006d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38eb4b-b6b2-46bd-92bc-6e2667ebb2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o1, o2 = mha_t(tensor, tensor, tensor, attn_mask=func(3), average_attn_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4e8c9-22dc-48c0-87d1-ac21e6a452ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039e93e-f344-4c5c-9822-5c2538ff0c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa151b-4ccf-485b-918f-812f178f6d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn2 = torch.nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454dec94-0627-4861-9335-a1ab2aaa89ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec1 = torch.tensor([1, 2])\n",
    "vec2 = torch.tensor([[0.4 , 0.3, 0.3], [0.2, 0.3, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c55730-742a-4595-ba07-24a0a571fae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4a855-8687-408a-b8ef-71a4d8ffb3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn2(vec2, vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68869e-158f-479b-985f-9f6484324799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6bb05-8587-4d74-bbb6-aea6634ab256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "\n",
    "    features, labels = data\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c708af-9e89-44d8-8ea3-a6efb6124bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model(features, labels[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88674094-5dbd-4a43-9c26-9615b6a7c92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out[0, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0756d-ac58-458b-9b0c-82d425555cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = loss_fn2(out.permute(0, 2, 1), labels[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fca79-7b6b-4ade-9a91-85784fe3e195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b981d-8156-4738-9513-8ddc1945ceb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels[:, 1:].contiguous().view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb38267-e4a2-41bf-90e9-7fda7557636b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.view(-1, out.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abec99-1673-4432-aaba-56fa3c6f1d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nonzero_indices = torch.nonzero(labels[:, 1:].contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c851573-4388-4018-9966-f2851332609e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = labels[:, 1:].contiguous().view(-1)[nonzero_indices].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e88f3f-438b-44f8-bdfc-4f8d0f0a8f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2 = out.view(-1, out.size(-1))[nonzero_indices][:].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25e0fc-990f-4534-8431-4ed6c3930353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8509cf87-76ae-408e-97df-41087404a418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3980d-dbdd-4a55-b9e6-e578dff9a977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn2(t2, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d24107-172d-4389-b7f7-adce6dbdbf40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
