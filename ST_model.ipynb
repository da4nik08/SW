{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8159cabd-100f-4427-8529-a859c288b103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "from collate import collate_fn\n",
    "from TdAtt import *\n",
    "from torch import nn\n",
    "import pickle\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4cb856-9b7f-49a4-9ccc-f33e12dea544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4059f5-6533-49ca-94dc-5340dd4f1cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84949da5-f671-44a4-a2df-ef0e4d791b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pkl(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402fc5b3-d569-4a13-82e3-9fe92619d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_wav, str_list = shuffle(get_pkl('mfcc.pkl'), get_pkl('str_list_cut.pkl'), random_state=0)\n",
    "np_wav = get_pkl('mfcc.pkl')\n",
    "str_list = get_pkl('str_list_cut.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4beeffa1-d731-4574-bedf-052abca07f7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "trainc = len(np_wav) - 500\n",
    "valc = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5f3e79-9944-4ac5-b6e3-27fed8749738",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainc = 3000\n",
    "valc = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4053f9db-7a63-4272-9491-14b0e5b9ed53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vnp_wav = np_wav[trainc:trainc+valc]\n",
    "vstr_list = str_list[trainc:trainc+valc]\n",
    "np_wav = np_wav[:trainc]\n",
    "str_list = str_list[:trainc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e66648f-8cca-41ab-8698-95e573c0fdee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(np_wav))\n",
    "print(len(str_list))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b8c6d1d-c095-45b3-bdcd-62072b40996e",
   "metadata": {
    "tags": []
   },
   "source": [
    "def pad_tensor(input_tensor, desired_length, fill_value=float('-inf')):\n",
    "    _, input_length, num_channels = input_tensor.size()\n",
    "\n",
    "    #padded_tensor = torch.nn.functional.pad(input_tensor, (0, 0, 0, pad_length))\n",
    "    pad_length = desired_length - input_length\n",
    "    padding = (0, 0, 0, pad_length)  # Pad (left, right, top, bottom)\n",
    "    padded_tensor = torch.nn.functional.pad(input_tensor, padding, value=fill_value)\n",
    "    \n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371b9def-1299-4657-9242-709a8406ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"\", \"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309963d1-a82c-405f-8507-233332e013a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 35\n",
      "[3, 12, 9, 29, 31, 27, 5, 29, 31, 13, 31, 11, 19, 24, 31, 5, 31, 18, 9, 27, 31, 7, 19, 17, 20, 16, 5, 13, 18, 24, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = VectorizeChar(400)\n",
    "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "print(vectorizer(\"hey way i got a new complaint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92fa4481-72a9-485f-88a4-c8c3c8731ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vect_str_list = [vectorizer(txt) for txt in str_list]\n",
    "vvect_str_list = [vectorizer(txt) for txt in vstr_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf89b50-fc99-4511-aeff-5983de30756d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=469):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]`` no\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb91f9da-2f3f-493d-9582-7114ef33d9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpeechFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim, num_2d, num_heads, num_hid=64, layernorm_eps=1e-6):\n",
    "        super(SpeechFeatureEmbedding, self).__init__()\n",
    "        self.num_layers = num_2d\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, num_hid, 3, padding=(0, 1), stride=(2, 1)),\n",
    "                             nn.BatchNorm2d(num_hid),\n",
    "                             nn.LeakyReLU(),\n",
    "                             #nn.Conv2d(num_hid, num_hid, 3, padding=(0, 1), stride=(2, 1)),\n",
    "                             #nn.BatchNorm2d(num_hid),\n",
    "                             #nn.ReLU()\n",
    "                            )\n",
    "        self.tda = nn.ModuleList([TwoD_Attention_layer(in_channels=num_hid, \n",
    "                                                    num_head=num_heads,\n",
    "                                                    emb_dim=embedding_dim,\n",
    "                                                    layernorm_eps=layernorm_eps) for _ in range(self.num_layers)])\n",
    "        self.lin = nn.Linear(embedding_dim * num_hid, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.tda[i](x)\n",
    "        x = self.lin(x.view(x.size(0), x.size(2), -1))\n",
    "        #return torch.squeeze(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca1fad77-5f14-4280-8f40-c274dabf76af",
   "metadata": {},
   "source": [
    "class SpeechFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim, num_hid=64):\n",
    "        super(SpeechFeatureEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, num_hid, 3, stride=(2, 2)),\n",
    "                             nn.BatchNorm2d(num_hid),\n",
    "                             nn.LeakyReLU(),\n",
    "                             nn.Conv2d(num_hid, num_hid, 3, stride=(2, 2)),\n",
    "                             nn.BatchNorm2d(num_hid),\n",
    "                             nn.ReLU()\n",
    "                            )\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(num_hid, 64, 3, padding=\"same\"),\n",
    "                             nn.BatchNorm2d(64),\n",
    "                             nn.LeakyReLU(),\n",
    "                            )\n",
    "        self.lin = nn.Linear(embedding_dim * num_hid, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        #x = self.conv3(x)\n",
    "        x = self.lin(x.view(x.size(0), x.size(2), -1))\n",
    "        #return torch.squeeze(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9775351-7a25-4a88-9e9b-2be1367c1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(embedding_dim, fully_connected_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(fully_connected_dim, embedding_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef8c131-9c36-4d9c-bf11-448f4e304d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self_mha_output, _ = self.mha(inputs, inputs, inputs)\n",
    "        \n",
    "        skip_attention = self.norm1(inputs + self_mha_output)\n",
    "        \n",
    "        ffn_output = self.ffn(skip_attention)\n",
    "        \n",
    "        ffn_output = self.dropout_ffn(ffn_output)\n",
    "        \n",
    "        encoder_layer_out = self.norm2(skip_attention + ffn_output)\n",
    "        \n",
    "        return encoder_layer_out        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a81a01-22da-4d86-9186-b2bf78dcec56",
   "metadata": {},
   "source": [
    "embedding_dim = d_model\n",
    "max_len = ntoken (time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0942e8a-e59f-4508-9178-c2278a237e99",
   "metadata": {},
   "source": [
    "возможно в конце linear слой для настройки ембедінг дім"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d34cf342-592c-472b-9901-31facd964c5c",
   "metadata": {},
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "               max_len, output_dim,  dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(self.embedding_dim, dropout_rate, max_len) # 3\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim[-1])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #x = inputs * math.sqrt(self.embedding_dim) #\n",
    "        x = self.pos_encoding(inputs) #\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = pad_tensor(x, self.output_dim[-2], fill_value=0.)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4a51ac4-1564-4ba7-8311-e8a86ca8f281",
   "metadata": {},
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ad709c-e680-4768-ae9c-6997c0b327a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "               max_len, output_dim,  dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.pad_length = self.output_dim[0] - max_len\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(self.embedding_dim, dropout_rate, max_len) # 3\n",
    "        \n",
    "        \"\"\"self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\"\"\"\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) for _ in range(self.num_layers)])\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, output_dim[-1]),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(max_len, output_dim[-2])\n",
    "            #nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def pad_tensor(self, input_tensor):\n",
    "\n",
    "        padded_tensor = torch.nn.functional.pad(input_tensor, (0, 0, 0, self.pad_length), value=0.)\n",
    "\n",
    "        return padded_tensor        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #x = inputs * math.sqrt(self.embedding_dim) #\n",
    "        x = self.pos_encoding(inputs) #\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        #x = self.linear(x)\n",
    "        #x = self.pad_tensor(x)\n",
    "        x = self.linear2(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c0f4-5a41-4f78-b74a-b91cd54af28e",
   "metadata": {},
   "source": [
    "внутрь декодера (не уровня) maxlen = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfe53555-31e5-4dc3-92a6-368c2b6b1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=35, maxlen=400, embedding_dim=64, dropout_rate=0.1):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.emb = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.pos_emb = PositionalEncoding(embedding_dim, dropout=0, max_len=maxlen) # d_model, dropout=0.1, max_len=469\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.emb(inputs)\n",
    "        #x = x * math.sqrt(self.embedding_dim)\n",
    "        x = self.pos_emb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c13de5c3-0c5a-4705-8560-f771fbe73ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm3 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, inputs, enc_output): # look_ahead_mask !!!!\n",
    "        \n",
    "        seq_len = inputs.size(1)\n",
    "        ahead_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).to(device)\n",
    "        #ahead_mask = self.create_look_ahead_mask2(seq_len)\n",
    "        \n",
    "        self_mha1_output, _ = self.mha1(inputs, inputs, inputs, attn_mask=ahead_mask) # look_ahead_mask !!!! batch\n",
    "        Q1 = self.norm1(self_mha1_output + inputs)\n",
    "        \n",
    "        self_mha2_output, _ = self.mha2(query=Q1, key=enc_output, value=enc_output) # pad mask  ???\n",
    "        skip_attention2 = self.norm2(self_mha2_output + Q1)\n",
    "        \n",
    "        ffn_output = self.ffn(skip_attention2)\n",
    "        drop_output = self.dropout_ffn(ffn_output)\n",
    "        skip3 = self.norm3(drop_output + skip_attention2)\n",
    "        \n",
    "        return skip3\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b29f0448-81f1-43c1-b603-462b31d1eade",
   "metadata": {},
   "source": [
    "def create_look_ahead_mask(self, sequence_length): # + batch size * num heads\n",
    "\n",
    "        mask = torch.tril(torch.ones((sequence_length, sequence_length))).to(device)\n",
    "        return mask\n",
    "    \n",
    "    def create_look_ahead_mask2(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.masked_fill(mask == 0, int(1)).masked_fill(mask == 1, int(0)).bool().to(device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44332f7c-5839-4572-9ec5-957e7908208a",
   "metadata": {},
   "source": [
    "embedding в decoder или в transformer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d4c8b72-4105-42cb-81ad-56c80e25166a",
   "metadata": {},
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,  #target_vocab_size, maximum_position_encoding,\n",
    "                 num_vocab=35, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_emb = TokenEmbedding(num_vocab=num_vocab, maxlen=maxlen, embedding_dim=embedding_dim) # num_vocab=34, maxlen=400, embedding_dim=64\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs, enc_output):\n",
    "        \n",
    "        x = self.token_emb(inputs) # torch.Size([Batch, 400, 64])\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output) # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        return x    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64092044-7045-4940-a447-3973471ce874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,  #target_vocab_size, maximum_position_encoding,\n",
    "                 num_vocab=35, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_emb = TokenEmbedding(num_vocab=num_vocab, maxlen=maxlen, embedding_dim=embedding_dim) # num_vocab=34, maxlen=400, embedding_dim=64\n",
    "        \n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) for _ in range(self.num_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs, enc_output):\n",
    "        \n",
    "        x = self.token_emb(inputs) # torch.Size([Batch, 400, 64])\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output) # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "230fa887-2b9a-4c00-855a-e4d9b8672911",
   "metadata": {},
   "source": [
    "self.layers = nn.ModuleList([\n",
    "            SpeechTransformerDecoderLayer(d_model, num_heads, d_ff, dropout_p, ffnet_style) for _ in range(num_layers)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c06673eb-7748-4abc-9d0d-e10a4b4f9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers_2d, num_layers_encoder, num_layers_decoder, embedding_dim_encoder, embedding_dim_decoder,\n",
    "                 num_heads_2d, num_heads_encoder, num_heads_decoder, fully_connected_dim_encoder, fully_connected_dim_decoder,\n",
    "                 target_vocab_size, max_len_enc, max_len_dec,\n",
    "                 enc_output_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.sfe = SpeechFeatureEmbedding(embedding_dim_encoder, enc_output_dim[-1], num_layers_2d, num_heads_2d, \n",
    "                                          layernorm_eps=layernorm_eps) # torch.Size([1, 116, 20])\n",
    "        \n",
    "        self.encoder = Encoder(num_layers=num_layers_encoder,\n",
    "                               embedding_dim=enc_output_dim[-1],\n",
    "                               #embedding_dim=embedding_dim_encoder,\n",
    "                               num_heads=num_heads_encoder,\n",
    "                               fully_connected_dim=fully_connected_dim_encoder,\n",
    "                               max_len=max_len_enc, # 116\n",
    "                               output_dim=enc_output_dim, # (400, 64)\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps) # torch.Size([1, 400, 64])\n",
    "        \n",
    "        \"\"\"encoder = Encoder(num_layers=2,\n",
    "                            embedding_dim=20,\n",
    "                            num_heads=10,\n",
    "                            fully_connected_dim=100,\n",
    "                            max_len=116,\n",
    "                            output_dim=(400, 64),\n",
    "                            dropout_rate=0)\"\"\"\n",
    "        \n",
    "        self.decoder = Decoder(num_layers=num_layers_decoder, \n",
    "                               embedding_dim=embedding_dim_decoder,\n",
    "                               num_heads=num_heads_decoder,\n",
    "                               fully_connected_dim=fully_connected_dim_decoder,\n",
    "                               num_vocab=target_vocab_size, # num_vocab=35\n",
    "                               maxlen=max_len_dec,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "        \"\"\"num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 num_vocab=34, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6\"\"\"\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.LazyLinear(target_vocab_size),\n",
    "            nn.Linear(embedding_dim_decoder, target_vocab_size),\n",
    "            #nn.Softmax(dim=-1) # 1\n",
    "            )\n",
    "        \n",
    "    def forward(self, input_spect_t, output_vect_str):\n",
    "        \n",
    "        enc_input = self.sfe(input_spect_t) # torch.Size([1, 116, 20]) 1 = N batches\n",
    "        \n",
    "        enc_output = self.encoder(enc_input)\n",
    "        \n",
    "        dec_output = self.decoder(output_vect_str, enc_output)  # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        final_output = self.linear(dec_output)\n",
    "        \n",
    "        return final_output  # [Batch, 400, 35]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e08ffe3-6809-4b4a-8e4f-c246bffe6a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(np_wav, vect_str_list)\n",
    "vdataset = CustomDataset(vnp_wav, vvect_str_list)\n",
    "dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate_fn, num_workers=2) # + num thread num_workers=6,\n",
    "vdataloader = DataLoader(vdataset, batch_size=20, collate_fn=collate_fn, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7851a51d-c392-41b9-b944-887656fb4661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, opt, loader):\n",
    "    loss_per_batches = 0\n",
    "    elapsed = 0\n",
    "    start_epoch2 = time.time()\n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        start_epoch = time.time()\n",
    "        features, labels = data\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        dec_input = labels[:, :-1]\n",
    "        dec_target = labels[:, 1:]\n",
    "        \n",
    "        y_pred = model(features, dec_input)\n",
    "        \n",
    "        #one_hot = nn.functional.one_hot(labels, 35).type(torch.float)\n",
    "        #indices = torch.nonzero(torch.eq(labels, 0))[0].item()\n",
    "        #print(str(labels) + \"y_pred\")\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        loss = loss_fn(y_pred.view(-1, y_pred.size(-1)), dec_target.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        loss_per_batches += loss\n",
    "        \n",
    "        end_epoch = time.time()\n",
    "        elapsed += (end_epoch - start_epoch)\n",
    "        \n",
    "    print(\"train = \" + str(elapsed))\n",
    "    print(\"train + load = \" + str(time.time() - start_epoch2))\n",
    "    return loss_per_batches/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ba98d9f-64f4-4d36-a1e0-77f91476f53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, opt, train_loader, val_loader, save_treshold=5, epochs=10, model_name='model_name'):\n",
    "        \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/' + model_name + '_{}'.format(timestamp))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = train_step(model, loss_fn, opt, train_loader)\n",
    "        model.eval()\n",
    "        \n",
    "        vloss = 0\n",
    "        counter = 0\n",
    "        with torch.inference_mode():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vfeatures, vlabels = vdata\n",
    "                vfeatures, vlabels = vfeatures.to(device), vlabels.to(device)\n",
    "                dec_input = vlabels[:, :-1]\n",
    "                dec_target = vlabels[:, 1:]\n",
    "\n",
    "                y_pred = model(vfeatures, dec_input)\n",
    "\n",
    "                vloss += loss_fn(y_pred.view(-1, y_pred.size(-1)), dec_target.contiguous().view(-1))\n",
    "                counter = i\n",
    "\n",
    "        avg_vloss = vloss / (counter + 1)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        \n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "        \n",
    "        if (epoch + 1) % save_treshold == 0:\n",
    "            model_path = model_name +'_{}_{}'.format(timestamp, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        end_epoch = time.time()\n",
    "        elapsed = end_epoch - start_epoch\n",
    "        print(\"Time per epoch {}s\".format(elapsed))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c093728-3229-40d5-b80b-4511f859a0de",
   "metadata": {},
   "source": [
    "                (self, num_layers_2d, num_layers_encoder, num_layers_decoder, embedding_dim_encoder, embedding_dim_decoder,\n",
    "                 num_heads_2d, num_heads_encoder, num_heads_decoder, fully_connected_dim_encoder, fully_connected_dim_decoder,\n",
    "                 target_vocab_size, max_len_enc, max_len_dec,\n",
    "                 enc_output_dim, dropout_rate=0.1, layernorm_eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45608453-8bfe-48fc-88e5-92329c4d6060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (sfe): SpeechFeatureEmbedding(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 1), padding=(0, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (tda): ModuleList(\n",
       "      (0-1): 2 x TwoD_Attention_layer(\n",
       "        (convq): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (convk): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (convv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (bnq): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bnk): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bnv): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (ln): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "        (final_conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (act1): ReLU()\n",
       "        (final_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (bnf1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bnf2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (lin): Linear(in_features=1280, out_features=64, bias=True)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (linear2): Sequential(\n",
       "      (0): Linear(in_features=234, out_features=399, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_emb): TokenEmbedding(\n",
       "      (emb): Embedding(35, 64)\n",
       "      (pos_emb): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dec_layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (mha1): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=35, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(2, 6, 3, 20, 64, #10, 8, 127, 64 (512)\n",
    "                32, 8, 8, 1024, 1024,\n",
    "                35, 234, 399, # 116\n",
    "                (399, 64),\n",
    "                dropout_rate=0.3)  # (, 64)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1754d2fb-6d7b-4124-bb61-18d3401779f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─SpeechFeatureEmbedding: 1-1                 --\n",
      "|    └─Sequential: 2-1                        --\n",
      "|    |    └─Conv2d: 3-1                       640\n",
      "|    |    └─BatchNorm2d: 3-2                  128\n",
      "|    |    └─LeakyReLU: 3-3                    --\n",
      "|    └─ModuleList: 2-2                        --\n",
      "|    |    └─TwoD_Attention_layer: 3-4         166,664\n",
      "|    |    └─TwoD_Attention_layer: 3-5         166,664\n",
      "|    └─Linear: 2-3                            81,984\n",
      "├─Encoder: 1-2                                --\n",
      "|    └─PositionalEncoding: 2-4                --\n",
      "|    |    └─Dropout: 3-6                      --\n",
      "|    └─ModuleList: 2-5                        --\n",
      "|    |    └─EncoderLayer: 3-7                 149,056\n",
      "|    |    └─EncoderLayer: 3-8                 149,056\n",
      "|    |    └─EncoderLayer: 3-9                 149,056\n",
      "|    |    └─EncoderLayer: 3-10                149,056\n",
      "|    |    └─EncoderLayer: 3-11                149,056\n",
      "|    |    └─EncoderLayer: 3-12                149,056\n",
      "|    └─Sequential: 2-6                        --\n",
      "|    |    └─Linear: 3-13                      4,160\n",
      "|    |    └─ReLU: 3-14                        --\n",
      "|    └─Sequential: 2-7                        --\n",
      "|    |    └─Linear: 3-15                      93,765\n",
      "├─Decoder: 1-3                                --\n",
      "|    └─TokenEmbedding: 2-8                    --\n",
      "|    |    └─Embedding: 3-16                   2,240\n",
      "|    |    └─PositionalEncoding: 3-17          --\n",
      "|    └─ModuleList: 2-9                        --\n",
      "|    |    └─DecoderLayer: 3-18                165,824\n",
      "|    |    └─DecoderLayer: 3-19                165,824\n",
      "|    |    └─DecoderLayer: 3-20                165,824\n",
      "|    └─Dropout: 2-10                          --\n",
      "├─Sequential: 1-4                             --\n",
      "|    └─Linear: 2-11                           2,275\n",
      "======================================================================\n",
      "Total params: 1,910,328\n",
      "Trainable params: 1,910,328\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "summary(model)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c3acadc-0eeb-4a92-ab33-dbe1f12b45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer2(nn.Module):\n",
    "    def __init__(self, target_vocab_size, d_model, nhead, num_layers):\n",
    "        super(Transformer2, self).__init__()\n",
    "\n",
    "        #self.encoder = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.encoder = SpeechFeatureEmbedding(20, d_model) \n",
    "        self.decoder = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout=0.2, max_len=400)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers[0],\n",
    "                                          num_decoder_layers=num_layers[1], dropout=0.2)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.encoder(src)\n",
    "        tgt_emb = self.decoder(tgt)\n",
    "        src_emb = self.pos_enc(src_emb)\n",
    "        tgt_emb = self.pos_enc(tgt_emb)\n",
    "        \n",
    "        src_emb = src_emb.permute(1, 0, 2)  # Change shape from [batch_size, seq_len_src, embedding_dim] to [seq_len_src, batch_size, embedding_dim]\n",
    "        tgt_emb = tgt_emb.permute(1, 0, 2)\n",
    "\n",
    "        memory = self.transformer.encoder(src_emb)\n",
    "\n",
    "        tgt_len = tgt_emb.size(0)\n",
    "        tgt_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool().to(device)\n",
    "\n",
    "        output = self.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "\n",
    "        output = output.permute(1, 0, 2)  # Change shape back to [batch_size, seq_len_tgt, d_model]\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eeb36273-2aed-4d87-bf3d-616733d44135",
   "metadata": {},
   "source": [
    "model2 = Transformer2(35, 64, 8, (6, 3))\n",
    "loss_fn2 = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ceaf449-4d96-4e5b-8763-5e5a9440a7ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "summary(model2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "091f5783-f7a5-4697-a7d8-b3e3079d1506",
   "metadata": {},
   "source": [
    "train(model2, loss_fn2, optimizer2, dataloader, vdataloader, epochs=100, model_name=model2.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3347f3-e9d6-4fc8-ade6-9a49518a0f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "train = 107.54716348648071\n",
      "train + load = 113.1325843334198\n",
      "LOSS train 2.6018409729003906 valid 2.4398040771484375\n",
      "Time per epoch 122.16564965248108s\n",
      "EPOCH 2:\n",
      "train = 103.28090357780457\n",
      "train + load = 109.19851589202881\n",
      "LOSS train 2.4420673847198486 valid 2.3944008350372314\n",
      "Time per epoch 118.130526304245s\n",
      "EPOCH 3:\n",
      "train = 103.60813355445862\n",
      "train + load = 109.48905944824219\n",
      "LOSS train 2.403848648071289 valid 2.3689873218536377\n",
      "Time per epoch 118.2852623462677s\n",
      "EPOCH 4:\n",
      "train = 103.63166046142578\n",
      "train + load = 109.38065981864929\n",
      "LOSS train 2.3804068565368652 valid 2.3469133377075195\n",
      "Time per epoch 118.39172959327698s\n",
      "EPOCH 5:\n",
      "train = 103.36095857620239\n",
      "train + load = 109.24494934082031\n",
      "LOSS train 2.3604836463928223 valid 2.3197081089019775\n",
      "Time per epoch 118.17771625518799s\n",
      "EPOCH 6:\n",
      "train = 103.40399241447449\n",
      "train + load = 109.50954723358154\n",
      "LOSS train 2.3410985469818115 valid 2.2951416969299316\n",
      "Time per epoch 118.34860396385193s\n",
      "EPOCH 7:\n",
      "train = 103.51073503494263\n",
      "train + load = 109.3707320690155\n",
      "LOSS train 2.3172030448913574 valid 2.253002643585205\n",
      "Time per epoch 118.2980706691742s\n",
      "EPOCH 8:\n",
      "train = 103.48771929740906\n",
      "train + load = 109.44347476959229\n",
      "LOSS train 2.2896735668182373 valid 2.215317726135254\n",
      "Time per epoch 118.42790651321411s\n",
      "EPOCH 9:\n",
      "train = 103.46459031105042\n",
      "train + load = 109.43670845031738\n",
      "LOSS train 2.2631843090057373 valid 2.172220468521118\n",
      "Time per epoch 118.3678150177002s\n",
      "EPOCH 10:\n",
      "train = 103.66963648796082\n",
      "train + load = 110.15875601768494\n",
      "LOSS train 2.236052989959717 valid 2.139892578125\n",
      "Time per epoch 119.31382393836975s\n",
      "EPOCH 11:\n",
      "train = 103.54398679733276\n",
      "train + load = 110.10447430610657\n",
      "LOSS train 2.2054426670074463 valid 2.1039481163024902\n",
      "Time per epoch 119.59107089042664s\n",
      "EPOCH 12:\n",
      "train = 103.64091920852661\n",
      "train + load = 112.76975560188293\n",
      "LOSS train 2.175189733505249 valid 2.0738918781280518\n",
      "Time per epoch 121.63247466087341s\n",
      "EPOCH 13:\n",
      "train = 103.65565586090088\n",
      "train + load = 109.62832689285278\n",
      "LOSS train 2.1476478576660156 valid 2.0411415100097656\n",
      "Time per epoch 118.36607193946838s\n",
      "EPOCH 14:\n",
      "train = 103.70986986160278\n",
      "train + load = 109.7020275592804\n",
      "LOSS train 2.1225228309631348 valid 2.0243351459503174\n",
      "Time per epoch 118.33736395835876s\n",
      "EPOCH 15:\n",
      "train = 103.3232479095459\n",
      "train + load = 109.30312824249268\n",
      "LOSS train 2.0972583293914795 valid 1.988523006439209\n",
      "Time per epoch 117.95741963386536s\n",
      "EPOCH 16:\n",
      "train = 96.79407596588135\n",
      "train + load = 102.52778697013855\n",
      "LOSS train 2.071885108947754 valid 1.976366400718689\n",
      "Time per epoch 110.95972776412964s\n",
      "EPOCH 17:\n",
      "train = 96.94473052024841\n",
      "train + load = 102.53657984733582\n",
      "LOSS train 2.0524985790252686 valid 1.9624103307724\n",
      "Time per epoch 111.48681783676147s\n",
      "EPOCH 18:\n",
      "train = 96.77685236930847\n",
      "train + load = 102.5683958530426\n",
      "LOSS train 2.0340702533721924 valid 1.945300579071045\n",
      "Time per epoch 111.71136140823364s\n",
      "EPOCH 19:\n",
      "train = 96.7336597442627\n",
      "train + load = 103.03403544425964\n",
      "LOSS train 2.016623020172119 valid 1.9370341300964355\n",
      "Time per epoch 112.23336219787598s\n",
      "EPOCH 20:\n",
      "train = 101.2556505203247\n",
      "train + load = 109.86094117164612\n",
      "LOSS train 2.0017881393432617 valid 1.917633295059204\n",
      "Time per epoch 118.5809109210968s\n",
      "EPOCH 21:\n",
      "train = 97.33184552192688\n",
      "train + load = 103.01371717453003\n",
      "LOSS train 1.9852319955825806 valid 1.9188045263290405\n",
      "Time per epoch 111.2854790687561s\n",
      "EPOCH 22:\n",
      "train = 96.94153094291687\n",
      "train + load = 102.21980023384094\n",
      "LOSS train 1.9707361459732056 valid 1.8944982290267944\n",
      "Time per epoch 110.50408101081848s\n",
      "EPOCH 23:\n",
      "train = 96.88443517684937\n",
      "train + load = 102.41127014160156\n",
      "LOSS train 1.9572522640228271 valid 1.8795831203460693\n",
      "Time per epoch 110.61824655532837s\n",
      "EPOCH 24:\n",
      "train = 96.77363896369934\n",
      "train + load = 102.11057448387146\n",
      "LOSS train 1.9465351104736328 valid 1.8731473684310913\n",
      "Time per epoch 110.72830247879028s\n",
      "EPOCH 25:\n",
      "train = 96.71400308609009\n",
      "train + load = 102.35128688812256\n",
      "LOSS train 1.9340399503707886 valid 1.8728383779525757\n",
      "Time per epoch 110.65051817893982s\n",
      "EPOCH 26:\n",
      "train = 96.69138860702515\n",
      "train + load = 102.16495990753174\n",
      "LOSS train 1.9220672845840454 valid 1.8586690425872803\n",
      "Time per epoch 110.48995733261108s\n",
      "EPOCH 27:\n",
      "train = 97.81373310089111\n",
      "train + load = 103.19167017936707\n",
      "LOSS train 1.913072109222412 valid 1.8446743488311768\n",
      "Time per epoch 111.7318696975708s\n",
      "EPOCH 28:\n",
      "train = 96.59889650344849\n",
      "train + load = 102.06198620796204\n",
      "LOSS train 1.9018646478652954 valid 1.836068034172058\n",
      "Time per epoch 110.378089427948s\n",
      "EPOCH 29:\n",
      "train = 96.81517291069031\n",
      "train + load = 102.19086527824402\n",
      "LOSS train 1.894195795059204 valid 1.839342474937439\n",
      "Time per epoch 110.43855714797974s\n",
      "EPOCH 30:\n",
      "train = 96.61123728752136\n",
      "train + load = 102.0995237827301\n",
      "LOSS train 1.8810807466506958 valid 1.8338810205459595\n",
      "Time per epoch 110.5674774646759s\n",
      "EPOCH 31:\n",
      "train = 96.88130807876587\n",
      "train + load = 102.30837225914001\n",
      "LOSS train 1.8751001358032227 valid 1.8274505138397217\n",
      "Time per epoch 110.55767583847046s\n",
      "EPOCH 32:\n",
      "train = 96.61836123466492\n",
      "train + load = 102.00920820236206\n",
      "LOSS train 1.8653473854064941 valid 1.814582347869873\n",
      "Time per epoch 110.26120805740356s\n",
      "EPOCH 33:\n",
      "train = 96.71850728988647\n",
      "train + load = 102.11297464370728\n",
      "LOSS train 1.8612632751464844 valid 1.8170539140701294\n",
      "Time per epoch 110.36707854270935s\n",
      "EPOCH 34:\n",
      "train = 96.64231777191162\n",
      "train + load = 102.10001230239868\n",
      "LOSS train 1.8515474796295166 valid 1.8105523586273193\n",
      "Time per epoch 110.39692878723145s\n",
      "EPOCH 35:\n",
      "train = 96.65356302261353\n",
      "train + load = 102.11534190177917\n",
      "LOSS train 1.8443841934204102 valid 1.805123209953308\n",
      "Time per epoch 110.42137169837952s\n",
      "EPOCH 36:\n",
      "train = 96.78321933746338\n",
      "train + load = 102.18545651435852\n",
      "LOSS train 1.8377584218978882 valid 1.8063820600509644\n",
      "Time per epoch 110.48455381393433s\n",
      "EPOCH 37:\n",
      "train = 96.87664818763733\n",
      "train + load = 102.32438945770264\n",
      "LOSS train 1.8308591842651367 valid 1.8005722761154175\n",
      "Time per epoch 110.97890281677246s\n",
      "EPOCH 38:\n",
      "train = 96.82647633552551\n",
      "train + load = 102.74514031410217\n",
      "LOSS train 1.8267948627471924 valid 1.7870162725448608\n",
      "Time per epoch 111.04049611091614s\n",
      "EPOCH 39:\n",
      "train = 98.53637409210205\n",
      "train + load = 104.04495191574097\n",
      "LOSS train 1.8189581632614136 valid 1.7847846746444702\n",
      "Time per epoch 112.44734191894531s\n",
      "EPOCH 40:\n",
      "train = 99.25320172309875\n",
      "train + load = 104.7723753452301\n",
      "LOSS train 1.811313509941101 valid 1.7827134132385254\n",
      "Time per epoch 113.62667298316956s\n",
      "EPOCH 41:\n",
      "train = 98.08012533187866\n",
      "train + load = 103.92116498947144\n",
      "LOSS train 1.807033896446228 valid 1.7737723588943481\n",
      "Time per epoch 112.37123012542725s\n",
      "EPOCH 42:\n",
      "train = 96.80364990234375\n",
      "train + load = 102.56857109069824\n",
      "LOSS train 1.8006502389907837 valid 1.77462637424469\n",
      "Time per epoch 110.77739071846008s\n",
      "EPOCH 43:\n",
      "train = 96.70022821426392\n",
      "train + load = 102.19506597518921\n",
      "LOSS train 1.7975558042526245 valid 1.7725130319595337\n",
      "Time per epoch 110.49469137191772s\n",
      "EPOCH 44:\n",
      "train = 96.75097799301147\n",
      "train + load = 102.13300824165344\n",
      "LOSS train 1.7906605005264282 valid 1.7662115097045898\n",
      "Time per epoch 110.40661287307739s\n",
      "EPOCH 45:\n",
      "train = 96.70225834846497\n",
      "train + load = 102.12482690811157\n",
      "LOSS train 1.7864102125167847 valid 1.7643500566482544\n",
      "Time per epoch 110.66201758384705s\n",
      "EPOCH 46:\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    train(model, loss_fn, optimizer, dataloader, vdataloader, epochs=500, model_name=model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bcb02-62f2-44ee-afda-6dded5a3c669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.triu(torch.ones(5, 5, dtype=torch.bool), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971acd6e-64bb-4ee0-9078-8c188bd0f868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1482345-fbad-4fc4-bd23-456e2ac161bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e5fbc-d9bb-4444-b694-5b4512a2e120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data_mfcc(np_wav, nperseg=1024, samplerate=24000):\n",
    "    mfcc = librosa.feature.mfcc(y=np_wav.astype(float), sr=samplerate, hop_length=nperseg)\n",
    "    pd = sklearn.preprocessing.scale(mfcc, axis=1)\n",
    "    new_shape = int(469 * 1024/nperseg)\n",
    "    pad = np.pad(pd, ((0, 0), (0, new_shape - pd.shape[1])), mode='constant')\n",
    "    return torch.tensor(np.expand_dims(np.swapaxes(pad,0,1), axis=0), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d4b8d-3b0e-4780-a7e5-648c0f5ff2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = torch.tensor([3]).to(device)\n",
    "dec_out = list()\n",
    "for i in range(400 - 1):\n",
    "    res = model(process_data_mfcc(np_wav[3]).unsqueeze(0).to(device), \n",
    "            inp.unsqueeze(0).to(device)).squeeze(0)\n",
    "    #print(len(inp))\n",
    "    soft_out = nn.functional.softmax(res, dim=-1)\n",
    "    last_logit = soft_out.argmax(dim=-1)[-1].unsqueeze(0)\n",
    "    #print(last_logit)\n",
    "    dec_out.append(last_logit)\n",
    "    inp = torch.cat((inp, last_logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03fe40-6004-4f6c-8cf4-e06dedd781ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = \"\"\n",
    "for x in inp:\n",
    "    out += voc[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb690a-dc94-448e-a67f-1a10dd27b996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4613d22-e584-4d80-b98a-586845078a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f5f13-f766-44a5-9a76-11115cfe5a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0d992-3c0a-49d1-aa9d-f2ec1d8681dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"0\" * 399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720efd7-9b73-47e7-bde4-bc76e1ab0d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd25f24-df2e-4080-961c-1dfb5356ff99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp2 = \"<\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba293db-64fd-4d61-a1c7-8899a2182476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac7f1b-8897-4b8e-b535-e151cd36fd26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = model(process_data_mfcc(vnp_wav[10]).unsqueeze(0).to(device), \n",
    "            torch.tensor(vvect_str_list[10])[:-1].unsqueeze(0).to(device)).squeeze(0)\n",
    "\n",
    "output_str = str()\n",
    "voc = vectorizer.get_vocabulary()\n",
    "\n",
    "inp = \"<\"\n",
    "for i in range(399):\n",
    "    output_str += voc[res[i].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13171cd-45c2-4ad2-bac0-0a423be6ea87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = model(process_data_mfcc(np_wav[0]).unsqueeze(0).to(device), \n",
    "            torch.tensor(vectorizer(\"<matth\"))[:-1].unsqueeze(0).to(device)).squeeze(0)\n",
    "\n",
    "output_str = str()\n",
    "voc = vectorizer.get_vocabulary()\n",
    "\n",
    "for i in range(399):\n",
    "    output_str += voc[res[i].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1d70b-9412-4a61-94a6-0ef1f8528db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20076383-698c-426c-8804-a077ba0c1eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42253f3d-0c90-4338-aef5-2d9962c0660f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_list[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fe2e9-da00-4d29-af9d-3daa96dd674a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_data_mfcc(np_wav[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02c289-bcba-4043-be53-a154c9881f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = torch.nonzero(torch.eq(tensor, 0))[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48baabcf-e896-43e0-9529-eee03154af5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdcca07-9f07-4cae-8111-d0e04bcc5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.empty(32, 400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226acd3-73ad-465e-a18b-b0e79f649acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tensor.view(-1, tensor.size(-1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc94c67-faf7-4025-898f-bbd4cd605b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf2021-ba53-4d86-b9f0-9b614b0ac369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabd59b-a717-4515-990b-cf2284509ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = process_data_mfcc(np_wav[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72665a-5e82-49e7-81ba-b1aaa5311c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa5f75-e3e3-4836-a529-7835920a8751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(sequence_length): # + batch size * num heads\n",
    "\n",
    "    mask = torch.tril(torch.ones((sequence_length, sequence_length)))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24572524-81c1-447b-b31e-4e55f16ead43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def func(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.masked_fill(mask == 0, int(1)).masked_fill(mask == 1, int(0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d00f4-36e6-4be0-b54a-90c31aeeb637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "func(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d51d1d-0510-4cd6-b0dc-ce1d463b8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_t = nn.MultiheadAttention(embed_dim=2, num_heads=1, dropout=0, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a9ae0-6731-443c-ba40-ca41ae1d115a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[[2, 3],[4, 5],[6, 7]]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e57f7c-490a-4def-9fff-5257da5006d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38eb4b-b6b2-46bd-92bc-6e2667ebb2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o1, o2 = mha_t(tensor, tensor, tensor, attn_mask=func(3), average_attn_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4e8c9-22dc-48c0-87d1-ac21e6a452ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039e93e-f344-4c5c-9822-5c2538ff0c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa151b-4ccf-485b-918f-812f178f6d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn2 = torch.nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454dec94-0627-4861-9335-a1ab2aaa89ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec1 = torch.tensor([1, 2])\n",
    "vec2 = torch.tensor([[0.4 , 0.3, 0.3], [0.2, 0.3, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c55730-742a-4595-ba07-24a0a571fae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4a855-8687-408a-b8ef-71a4d8ffb3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn2(vec2, vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68869e-158f-479b-985f-9f6484324799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6bb05-8587-4d74-bbb6-aea6634ab256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "\n",
    "    features, labels = data\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c708af-9e89-44d8-8ea3-a6efb6124bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model(features, labels[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88674094-5dbd-4a43-9c26-9615b6a7c92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out[0, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0756d-ac58-458b-9b0c-82d425555cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = loss_fn2(out.permute(0, 2, 1), labels[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fca79-7b6b-4ade-9a91-85784fe3e195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b981d-8156-4738-9513-8ddc1945ceb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels[:, 1:].contiguous().view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb38267-e4a2-41bf-90e9-7fda7557636b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.view(-1, out.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abec99-1673-4432-aaba-56fa3c6f1d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nonzero_indices = torch.nonzero(labels[:, 1:].contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c851573-4388-4018-9966-f2851332609e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = labels[:, 1:].contiguous().view(-1)[nonzero_indices].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e88f3f-438b-44f8-bdfc-4f8d0f0a8f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2 = out.view(-1, out.size(-1))[nonzero_indices][:].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25e0fc-990f-4534-8431-4ed6c3930353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8509cf87-76ae-408e-97df-41087404a418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3980d-dbdd-4a55-b9e6-e578dff9a977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn2(t2, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d24107-172d-4389-b7f7-adce6dbdbf40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
