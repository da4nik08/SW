{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8159cabd-100f-4427-8529-a859c288b103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "from collate import collate_fn\n",
    "from torch import nn\n",
    "import pickle\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4cb856-9b7f-49a4-9ccc-f33e12dea544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4059f5-6533-49ca-94dc-5340dd4f1cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84949da5-f671-44a4-a2df-ef0e4d791b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pkl(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402fc5b3-d569-4a13-82e3-9fe92619d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_wav = get_pkl('np_wavs_cut.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4053f9db-7a63-4272-9491-14b0e5b9ed53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np_wav = np_wav[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d09e58-acd5-4076-8537-70f8c2ed8c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_list = get_pkl('str_list_cut.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6444c7ba-e462-4bdc-a8e0-c8585a555b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = str_list[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e66648f-8cca-41ab-8698-95e573c0fdee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(np_wav))\n",
    "print(len(str_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662809e7-40bd-4fee-99fa-2df8a8506549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_tensor(input_tensor, desired_length, fill_value=float('-inf')):\n",
    "    _, input_length, num_channels = input_tensor.size()\n",
    "\n",
    "    #padded_tensor = torch.nn.functional.pad(input_tensor, (0, 0, 0, pad_length))\n",
    "    pad_length = desired_length - input_length\n",
    "    padding = (0, 0, 0, pad_length)  # Pad (left, right, top, bottom)\n",
    "    padded_tensor = torch.nn.functional.pad(input_tensor, padding, value=fill_value)\n",
    "    \n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "371b9def-1299-4657-9242-709a8406ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"\", \"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "309963d1-a82c-405f-8507-233332e013a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 35\n",
      "[3, 12, 9, 29, 31, 27, 5, 29, 31, 13, 31, 11, 19, 24, 31, 5, 31, 18, 9, 27, 31, 7, 19, 17, 20, 16, 5, 13, 18, 24, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = VectorizeChar(400)\n",
    "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "print(vectorizer(\"hey way i got a new complaint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92fa4481-72a9-485f-88a4-c8c3c8731ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vect_str_list = [vectorizer(txt) for txt in str_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddf89b50-fc99-4511-aeff-5983de30756d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=469):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]`` no\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d9babdc-484a-44e9-87a2-ec92401b04fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpeechFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_hid=64):\n",
    "        super(SpeechFeatureEmbedding, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, num_hid, 3, padding=(0, 1), stride=(2, 1)),\n",
    "                             nn.BatchNorm2d(num_hid),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Conv2d(num_hid, num_hid, 3, padding=(0, 1), stride=(2, 1)),\n",
    "                             nn.BatchNorm2d(num_hid),\n",
    "                             nn.ReLU()\n",
    "                            )\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(num_hid, 1, 1, padding=\"same\"),\n",
    "                             nn.BatchNorm2d(1),\n",
    "                             nn.ReLU(),\n",
    "                            )\n",
    "        self.lin = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.lin(x)\n",
    "        return torch.squeeze(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9775351-7a25-4a88-9e9b-2be1367c1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(embedding_dim, fully_connected_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(fully_connected_dim, embedding_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ef8c131-9c36-4d9c-bf11-448f4e304d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self_mha_output, _ = self.mha(inputs, inputs, inputs)\n",
    "        \n",
    "        skip_attention = self.norm1(torch.add(inputs, self_mha_output))\n",
    "        \n",
    "        ffn_output = self.ffn(skip_attention)\n",
    "        \n",
    "        ffn_output = self.dropout_ffn(ffn_output)\n",
    "        \n",
    "        encoder_layer_out = self.norm2(torch.add(skip_attention, ffn_output))\n",
    "        \n",
    "        return encoder_layer_out        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a81a01-22da-4d86-9186-b2bf78dcec56",
   "metadata": {},
   "source": [
    "embedding_dim = d_model\n",
    "max_len = ntoken (time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0942e8a-e59f-4508-9178-c2278a237e99",
   "metadata": {},
   "source": [
    "возможно в конце linear слой для настройки ембедінг дім"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88fbfd1c-0c0e-4143-b338-6ef0912e3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "               max_len, output_dim,  dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(self.embedding_dim, dropout_rate, max_len) # 3\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim[-1])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #x = inputs * math.sqrt(self.embedding_dim) #\n",
    "        x = self.pos_encoding(inputs) #\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = pad_tensor(x, self.output_dim[-2], fill_value=0.)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c0f4-5a41-4f78-b74a-b91cd54af28e",
   "metadata": {},
   "source": [
    "внутрь декодера (не уровня) maxlen = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe53555-31e5-4dc3-92a6-368c2b6b1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=35, maxlen=400, embedding_dim=64, dropout_rate=0.1):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.emb = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.pos_emb = PositionalEncoding(embedding_dim, dropout=0, max_len=maxlen) # d_model, dropout=0.1, max_len=469\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.emb(inputs)\n",
    "        #x = x * math.sqrt(self.embedding_dim)\n",
    "        x = self.pos_emb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c13de5c3-0c5a-4705-8560-f771fbe73ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm3 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def create_look_ahead_mask(self, sequence_length): # + batch size * num heads\n",
    "\n",
    "        mask = torch.tril(torch.ones((sequence_length, sequence_length)))\n",
    "        return mask.to(device)\n",
    "\n",
    "    def forward(self, inputs, enc_output): # look_ahead_mask !!!!\n",
    "        \n",
    "        seq_len = inputs.size(1)\n",
    "        ahead_mask = self.create_look_ahead_mask(seq_len)\n",
    "        \n",
    "        self_mha1_output, _ = self.mha1(inputs, inputs, inputs, attn_mask=ahead_mask) # look_ahead_mask !!!! batch\n",
    "        Q1 = self.norm1(torch.add(self_mha1_output, inputs))\n",
    "        \n",
    "        self_mha2_output, _ = self.mha2(query=Q1, key=enc_output, value=enc_output) # pad mask  ???\n",
    "        skip_attention2 = self.norm2(torch.add(self_mha2_output, Q1))\n",
    "        \n",
    "        ffn_output = self.ffn(skip_attention2)\n",
    "        drop_output = self.dropout_ffn(ffn_output)\n",
    "        skip3 = self.norm3(drop_output)\n",
    "        \n",
    "        return skip3\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "44332f7c-5839-4572-9ec5-957e7908208a",
   "metadata": {},
   "source": [
    "embedding в decoder или в transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49afe50d-1e16-4887-9539-f599ae67fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,  #target_vocab_size, maximum_position_encoding,\n",
    "                 num_vocab=35, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_emb = TokenEmbedding(num_vocab=num_vocab, maxlen=maxlen, embedding_dim=embedding_dim) # num_vocab=34, maxlen=400, embedding_dim=64\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs, enc_output):\n",
    "        \n",
    "        x = self.token_emb(inputs) # torch.Size([Batch, 400, 64])\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output) # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        return x    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c06673eb-7748-4abc-9d0d-e10a4b4f9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers_encoder, num_layers_decoder, embedding_dim_encoder, embedding_dim_decoder,\n",
    "                 num_heads_encoder, num_heads_decoder, fully_connected_dim_encoder, fully_connected_dim_decoder,\n",
    "                 target_vocab_size, max_len_enc, max_len_dec,\n",
    "                 enc_output_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.sfe = SpeechFeatureEmbedding(embedding_dim_encoder) # torch.Size([1, 116, 20])\n",
    "        \n",
    "        self.encoder = Encoder(num_layers=num_layers_encoder,\n",
    "                               embedding_dim=embedding_dim_encoder,\n",
    "                               num_heads=num_heads_encoder,\n",
    "                               fully_connected_dim=fully_connected_dim_encoder,\n",
    "                               max_len=max_len_enc, # 116\n",
    "                               output_dim=enc_output_dim, # (400, 64)\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps) # torch.Size([1, 400, 64])\n",
    "        \n",
    "        \"\"\"encoder = Encoder(num_layers=2,\n",
    "                            embedding_dim=20,\n",
    "                            num_heads=10,\n",
    "                            fully_connected_dim=100,\n",
    "                            max_len=116,\n",
    "                            output_dim=(400, 64),\n",
    "                            dropout_rate=0)\"\"\"\n",
    "        \n",
    "        self.decoder = Decoder(num_layers=num_layers_decoder, \n",
    "                               embedding_dim=embedding_dim_decoder,\n",
    "                               num_heads=num_heads_decoder,\n",
    "                               fully_connected_dim=fully_connected_dim_decoder,\n",
    "                               num_vocab=target_vocab_size, # num_vocab=35\n",
    "                               maxlen=max_len_dec,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "        \"\"\"num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 num_vocab=34, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6\"\"\"\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.LazyLinear(target_vocab_size),\n",
    "            nn.Linear(embedding_dim_decoder, target_vocab_size),\n",
    "            nn.Softmax(dim=1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, input_spect_t, output_vect_str):\n",
    "        \n",
    "        enc_input = self.sfe(input_spect_t) # torch.Size([1, 116, 20]) 1 = N batches\n",
    "        \n",
    "        enc_output = self.encoder(enc_input)\n",
    "        \n",
    "        dec_output = self.decoder(output_vect_str, enc_output)  # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        final_output = self.linear(dec_output)\n",
    "        \n",
    "        return final_output  # [Batch, 400, 35]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e08ffe3-6809-4b4a-8e4f-c246bffe6a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(np_wav, vect_str_list)\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, num_workers=3) # + num thread num_workers=6,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7851a51d-c392-41b9-b944-887656fb4661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, opt, loader):\n",
    "    loss_per_batches = 0\n",
    "    elapsed = 0\n",
    "    start_epoch2 = time.time()\n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        start_epoch = time.time()\n",
    "        features, labels = data\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        y_pred = model(features, labels)\n",
    "        \n",
    "        one_hot = nn.functional.one_hot(labels, 35).type(torch.float)\n",
    "        \n",
    "        loss = loss_fn(y_pred, one_hot)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        loss_per_batches += loss\n",
    "        \n",
    "        end_epoch = time.time()\n",
    "        elapsed += (end_epoch - start_epoch)\n",
    "        \n",
    "    print(\"train = \" + str(elapsed))\n",
    "    print(\"train + load = \" + str(time.time() - start_epoch2))\n",
    "    return loss_per_batches/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ba98d9f-64f4-4d36-a1e0-77f91476f53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, opt, train_loader, save_treshold=5, epochs=10, model_name='model_name'):\n",
    "        \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/' + model_name + '_{}'.format(timestamp))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = train_step(model, loss_fn, opt, train_loader)\n",
    "        model.eval()\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print('LOSS train {}'.format(avg_loss))\n",
    "        \n",
    "        writer.add_scalars('Training Loss',\n",
    "                    { 'Training' : avg_loss },\n",
    "                    epoch + 1)\n",
    "        \n",
    "        if (epoch + 1) % save_treshold == 0:\n",
    "            model_path = model_name +'_{}_{}'.format(timestamp, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        end_epoch = time.time()\n",
    "        elapsed = end_epoch - start_epoch\n",
    "        print(\"Time per epoch {}s\".format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975dcfa-eac8-4b43-bfe1-1fc546c82160",
   "metadata": {},
   "source": [
    "                (self, num_layers_encoder, num_layers_decoder, embedding_dim_encoder, embedding_dim_decoder,\n",
    "                 num_heads_encoder, num_heads_decoder, fully_connected_dim_encoder, fully_connected_dim_decoder,\n",
    "                 target_vocab_size, max_len_enc, max_len_dec,\n",
    "                 enc_output_dim, dropout_rate=0.1, layernorm_eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45608453-8bfe-48fc-88e5-92329c4d6060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (sfe): SpeechFeatureEmbedding(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 1), padding=(0, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 1), padding=(0, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (lin): Linear(in_features=20, out_features=20, bias=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (linear): Linear(in_features=20, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_emb): TokenEmbedding(\n",
       "      (emb): Embedding(35, 32)\n",
       "      (pos_emb): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=35, bias=True)\n",
       "    (1): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(4, 2, 20, 32, #10, 8, , 64\n",
    "                10, 8, 100, 100,\n",
    "                35, 116, 400,\n",
    "                (400, 32))  # (, 64)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c3347f3-e9d6-4fc8-ade6-9a49518a0f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "train = 13.206430912017822\n",
      "train + load = 24.45325803756714\n",
      "LOSS train 68.47205352783203\n",
      "Time per epoch 24.457258939743042s\n",
      "EPOCH 2:\n",
      "train = 4.839326858520508\n",
      "train + load = 15.484957218170166\n",
      "LOSS train 68.46662139892578\n",
      "Time per epoch 15.49595832824707s\n",
      "EPOCH 3:\n",
      "train = 4.833075046539307\n",
      "train + load = 15.409869194030762\n",
      "LOSS train 68.45516204833984\n",
      "Time per epoch 15.42087173461914s\n",
      "EPOCH 4:\n",
      "train = 4.852826118469238\n",
      "train + load = 15.201218128204346\n",
      "LOSS train 68.42826080322266\n",
      "Time per epoch 15.209206342697144s\n",
      "EPOCH 5:\n",
      "train = 4.953278541564941\n",
      "train + load = 15.008864164352417\n",
      "LOSS train 68.37508392333984\n",
      "Time per epoch 15.076348066329956s\n",
      "EPOCH 6:\n",
      "train = 4.977241039276123\n",
      "train + load = 15.392274856567383\n",
      "LOSS train 68.2989730834961\n",
      "Time per epoch 15.399282217025757s\n",
      "EPOCH 7:\n",
      "train = 5.035881757736206\n",
      "train + load = 15.49367880821228\n",
      "LOSS train 68.21434783935547\n",
      "Time per epoch 15.501662254333496s\n",
      "EPOCH 8:\n",
      "train = 4.959097623825073\n",
      "train + load = 15.278133869171143\n",
      "LOSS train 68.1369400024414\n",
      "Time per epoch 15.286129236221313s\n",
      "EPOCH 9:\n",
      "train = 4.849408388137817\n",
      "train + load = 15.271049737930298\n",
      "LOSS train 68.07335662841797\n",
      "Time per epoch 15.279045820236206s\n",
      "EPOCH 10:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loss_fn, opt, train_loader, save_treshold, epochs, model_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 12\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     15\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(avg_loss)\n",
      "Cell \u001b[1;32mIn[24], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, loss_fn, opt, loader)\u001b[0m\n\u001b[0;32m     14\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(labels, \u001b[38;5;241m35\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, one_hot)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     21\u001b[0m loss_per_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, loss_fn, optimizer, dataloader, epochs=100, model_name=model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807bca0-680e-4c63-83f7-a3d4d2274cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
