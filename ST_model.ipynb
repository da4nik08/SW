{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8159cabd-100f-4427-8529-a859c288b103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from custom_dataset import CustomDataset\n",
    "from collate import collate_fn\n",
    "from torch import nn\n",
    "import pickle\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4cb856-9b7f-49a4-9ccc-f33e12dea544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4059f5-6533-49ca-94dc-5340dd4f1cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84949da5-f671-44a4-a2df-ef0e4d791b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pkl(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402fc5b3-d569-4a13-82e3-9fe92619d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_wav = get_pkl('np_wavs_cut.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4053f9db-7a63-4272-9491-14b0e5b9ed53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np_wav = np_wav[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d09e58-acd5-4076-8537-70f8c2ed8c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_list = get_pkl('str_list_cut.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6444c7ba-e462-4bdc-a8e0-c8585a555b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = str_list[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e66648f-8cca-41ab-8698-95e573c0fdee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(np_wav))\n",
    "print(len(str_list))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b8c6d1d-c095-45b3-bdcd-62072b40996e",
   "metadata": {
    "tags": []
   },
   "source": [
    "def pad_tensor(input_tensor, desired_length, fill_value=float('-inf')):\n",
    "    _, input_length, num_channels = input_tensor.size()\n",
    "\n",
    "    #padded_tensor = torch.nn.functional.pad(input_tensor, (0, 0, 0, pad_length))\n",
    "    pad_length = desired_length - input_length\n",
    "    padding = (0, 0, 0, pad_length)  # Pad (left, right, top, bottom)\n",
    "    padded_tensor = torch.nn.functional.pad(input_tensor, padding, value=fill_value)\n",
    "    \n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371b9def-1299-4657-9242-709a8406ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"\", \"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "309963d1-a82c-405f-8507-233332e013a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 35\n",
      "[3, 12, 9, 29, 31, 27, 5, 29, 31, 13, 31, 11, 19, 24, 31, 5, 31, 18, 9, 27, 31, 7, 19, 17, 20, 16, 5, 13, 18, 24, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = VectorizeChar(400)\n",
    "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "print(vectorizer(\"hey way i got a new complaint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92fa4481-72a9-485f-88a4-c8c3c8731ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vect_str_list = [vectorizer(txt) for txt in str_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf89b50-fc99-4511-aeff-5983de30756d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=469):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]`` no\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d9babdc-484a-44e9-87a2-ec92401b04fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpeechFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_hid=64):\n",
    "        super(SpeechFeatureEmbedding, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, num_hid, 3, padding=(0, 1), stride=(2, 1)),\n",
    "                             nn.BatchNorm2d(num_hid),\n",
    "                             nn.LeakyReLU(),\n",
    "                             #nn.Conv2d(num_hid, num_hid, 3, padding=(0, 1), stride=(2, 1)),\n",
    "                             #nn.BatchNorm2d(num_hid),\n",
    "                             #nn.ReLU()\n",
    "                            )\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(num_hid, 1, 1, padding=\"same\"),\n",
    "                             nn.BatchNorm2d(1),\n",
    "                             nn.LeakyReLU(),\n",
    "                            )\n",
    "        #self.lin = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv3(x)\n",
    "        #x = self.lin(x)\n",
    "        return torch.squeeze(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9775351-7a25-4a88-9e9b-2be1367c1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(embedding_dim, fully_connected_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(fully_connected_dim, embedding_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ef8c131-9c36-4d9c-bf11-448f4e304d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self_mha_output, _ = self.mha(inputs, inputs, inputs)\n",
    "        \n",
    "        skip_attention = self.norm1(inputs + self_mha_output)\n",
    "        \n",
    "        ffn_output = self.ffn(skip_attention)\n",
    "        \n",
    "        ffn_output = self.dropout_ffn(ffn_output)\n",
    "        \n",
    "        encoder_layer_out = self.norm2(skip_attention + ffn_output)\n",
    "        \n",
    "        return encoder_layer_out        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a81a01-22da-4d86-9186-b2bf78dcec56",
   "metadata": {},
   "source": [
    "embedding_dim = d_model\n",
    "max_len = ntoken (time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0942e8a-e59f-4508-9178-c2278a237e99",
   "metadata": {},
   "source": [
    "возможно в конце linear слой для настройки ембедінг дім"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d34cf342-592c-472b-9901-31facd964c5c",
   "metadata": {},
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "               max_len, output_dim,  dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(self.embedding_dim, dropout_rate, max_len) # 3\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim[-1])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #x = inputs * math.sqrt(self.embedding_dim) #\n",
    "        x = self.pos_encoding(inputs) #\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = pad_tensor(x, self.output_dim[-2], fill_value=0.)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7784fb40-b4ab-4f0f-a74d-7a4836f64f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5ad709c-e680-4768-ae9c-6997c0b327a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "               max_len, output_dim,  dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.pad_length = self.output_dim[0] - max_len\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(self.embedding_dim, dropout_rate, max_len) # 3\n",
    "        \n",
    "        \"\"\"self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\"\"\"\n",
    "        self.enc_layers = get_clones(EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps), self.num_layers)\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim[-1])\n",
    "\n",
    "    def pad_tensor(self, input_tensor):\n",
    "\n",
    "        padded_tensor = torch.nn.functional.pad(input_tensor, (0, 0, 0, self.pad_length), value=0.)\n",
    "\n",
    "        return padded_tensor        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #x = inputs * math.sqrt(self.embedding_dim) #\n",
    "        x = self.pos_encoding(inputs) #\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = self.pad_tensor(x)\n",
    "        #x = self.pad_tensor(x, self.output_dim[-2], fill_value=0.)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c0f4-5a41-4f78-b74a-b91cd54af28e",
   "metadata": {},
   "source": [
    "внутрь декодера (не уровня) maxlen = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe53555-31e5-4dc3-92a6-368c2b6b1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=35, maxlen=400, embedding_dim=64, dropout_rate=0.1):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.emb = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.pos_emb = PositionalEncoding(embedding_dim, dropout=0, max_len=maxlen) # d_model, dropout=0.1, max_len=469\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.emb(inputs)\n",
    "        #x = x * math.sqrt(self.embedding_dim)\n",
    "        x = self.pos_emb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c13de5c3-0c5a-4705-8560-f771fbe73ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm3 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def create_look_ahead_mask(self, sequence_length): # + batch size * num heads\n",
    "\n",
    "        mask = torch.tril(torch.ones((sequence_length, sequence_length))).to(device)\n",
    "        return mask\n",
    "    \n",
    "    def create_look_ahead_mask2(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.masked_fill(mask == 0, int(1)).masked_fill(mask == 1, int(0)).to(device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, inputs, enc_output): # look_ahead_mask !!!!\n",
    "        \n",
    "        seq_len = inputs.size(1)\n",
    "        ahead_mask = self.create_look_ahead_mask2(seq_len)\n",
    "        \n",
    "        self_mha1_output, _ = self.mha1(inputs, inputs, inputs, attn_mask=ahead_mask) # look_ahead_mask !!!! batch\n",
    "        Q1 = self.norm1(self_mha1_output + inputs)\n",
    "        \n",
    "        self_mha2_output, _ = self.mha2(query=Q1, key=enc_output, value=enc_output) # pad mask  ???\n",
    "        skip_attention2 = self.norm2(self_mha2_output + Q1)\n",
    "        \n",
    "        ffn_output = self.ffn(skip_attention2)\n",
    "        drop_output = self.dropout_ffn(ffn_output)\n",
    "        skip3 = self.norm3(drop_output + skip_attention2)\n",
    "        \n",
    "        return skip3\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "44332f7c-5839-4572-9ec5-957e7908208a",
   "metadata": {},
   "source": [
    "embedding в decoder или в transformer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d4c8b72-4105-42cb-81ad-56c80e25166a",
   "metadata": {},
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,  #target_vocab_size, maximum_position_encoding,\n",
    "                 num_vocab=35, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_emb = TokenEmbedding(num_vocab=num_vocab, maxlen=maxlen, embedding_dim=embedding_dim) # num_vocab=34, maxlen=400, embedding_dim=64\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps).to(device) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs, enc_output):\n",
    "        \n",
    "        x = self.token_emb(inputs) # torch.Size([Batch, 400, 64])\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output) # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        return x    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64092044-7045-4940-a447-3973471ce874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,  #target_vocab_size, maximum_position_encoding,\n",
    "                 num_vocab=35, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_emb = TokenEmbedding(num_vocab=num_vocab, maxlen=maxlen, embedding_dim=embedding_dim) # num_vocab=34, maxlen=400, embedding_dim=64\n",
    "        \n",
    "        self.dec_layers = get_clones(DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps), self.num_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs, enc_output):\n",
    "        \n",
    "        x = self.token_emb(inputs) # torch.Size([Batch, 400, 64])\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output) # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "230fa887-2b9a-4c00-855a-e4d9b8672911",
   "metadata": {},
   "source": [
    "self.layers = nn.ModuleList([\n",
    "            SpeechTransformerDecoderLayer(d_model, num_heads, d_ff, dropout_p, ffnet_style) for _ in range(num_layers)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c06673eb-7748-4abc-9d0d-e10a4b4f9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers_encoder, num_layers_decoder, embedding_dim_encoder, embedding_dim_decoder,\n",
    "                 num_heads_encoder, num_heads_decoder, fully_connected_dim_encoder, fully_connected_dim_decoder,\n",
    "                 target_vocab_size, max_len_enc, max_len_dec,\n",
    "                 enc_output_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.sfe = SpeechFeatureEmbedding(embedding_dim_encoder) # torch.Size([1, 116, 20])\n",
    "        \n",
    "        self.encoder = Encoder(num_layers=num_layers_encoder,\n",
    "                               embedding_dim=embedding_dim_encoder,\n",
    "                               num_heads=num_heads_encoder,\n",
    "                               fully_connected_dim=fully_connected_dim_encoder,\n",
    "                               max_len=max_len_enc, # 116\n",
    "                               output_dim=enc_output_dim, # (400, 64)\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps) # torch.Size([1, 400, 64])\n",
    "        \n",
    "        \"\"\"encoder = Encoder(num_layers=2,\n",
    "                            embedding_dim=20,\n",
    "                            num_heads=10,\n",
    "                            fully_connected_dim=100,\n",
    "                            max_len=116,\n",
    "                            output_dim=(400, 64),\n",
    "                            dropout_rate=0)\"\"\"\n",
    "        \n",
    "        self.decoder = Decoder(num_layers=num_layers_decoder, \n",
    "                               embedding_dim=embedding_dim_decoder,\n",
    "                               num_heads=num_heads_decoder,\n",
    "                               fully_connected_dim=fully_connected_dim_decoder,\n",
    "                               num_vocab=target_vocab_size, # num_vocab=35\n",
    "                               maxlen=max_len_dec,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "        \"\"\"num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 num_vocab=34, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6\"\"\"\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.LazyLinear(target_vocab_size),\n",
    "            nn.Linear(embedding_dim_decoder, target_vocab_size),\n",
    "            #nn.Softmax(dim=-1) # 1\n",
    "            )\n",
    "        \n",
    "    def forward(self, input_spect_t, output_vect_str):\n",
    "        \n",
    "        enc_input = self.sfe(input_spect_t) # torch.Size([1, 116, 20]) 1 = N batches\n",
    "        \n",
    "        enc_output = self.encoder(enc_input)\n",
    "        \n",
    "        dec_output = self.decoder(output_vect_str, enc_output)  # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        final_output = self.linear(dec_output)\n",
    "        \n",
    "        return final_output  # [Batch, 400, 35]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e08ffe3-6809-4b4a-8e4f-c246bffe6a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(np_wav, vect_str_list)\n",
    "dataloader = DataLoader(dataset, batch_size=16, collate_fn=collate_fn, num_workers=3) # + num thread num_workers=6,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7851a51d-c392-41b9-b944-887656fb4661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, opt, loader):\n",
    "    loss_per_batches = 0\n",
    "    elapsed = 0\n",
    "    start_epoch2 = time.time()\n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        start_epoch = time.time()\n",
    "        features, labels = data\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        dec_input = labels[:, :-1]\n",
    "        dec_target = labels[:, 1:]\n",
    "        \n",
    "        y_pred = model(features, dec_input)\n",
    "        \n",
    "        #one_hot = nn.functional.one_hot(labels, 35).type(torch.float)\n",
    "        #indices = torch.nonzero(torch.eq(labels, 0))[0].item()\n",
    "        #print(str(labels) + \"y_pred\")\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        loss = loss_fn(y_pred.view(-1, y_pred.size(-1)), dec_target.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        loss_per_batches += loss\n",
    "        \n",
    "        end_epoch = time.time()\n",
    "        elapsed += (end_epoch - start_epoch)\n",
    "        \n",
    "    print(\"train = \" + str(elapsed))\n",
    "    print(\"train + load = \" + str(time.time() - start_epoch2))\n",
    "    return loss_per_batches/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ba98d9f-64f4-4d36-a1e0-77f91476f53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, opt, train_loader, save_treshold=5, epochs=10, model_name='model_name'):\n",
    "        \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/' + model_name + '_{}'.format(timestamp))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=10, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = train_step(model, loss_fn, opt, train_loader)\n",
    "        model.eval()\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print('LOSS train {}'.format(avg_loss))\n",
    "        \n",
    "        writer.add_scalars('Training Loss',\n",
    "                    { 'Training' : avg_loss },\n",
    "                    epoch + 1)\n",
    "        \n",
    "        if (epoch + 1) % save_treshold == 0:\n",
    "            model_path = model_name +'_{}_{}'.format(timestamp, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        end_epoch = time.time()\n",
    "        elapsed = end_epoch - start_epoch\n",
    "        print(\"Time per epoch {}s\".format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975dcfa-eac8-4b43-bfe1-1fc546c82160",
   "metadata": {},
   "source": [
    "                (self, num_layers_encoder, num_layers_decoder, embedding_dim_encoder, embedding_dim_decoder,\n",
    "                 num_heads_encoder, num_heads_decoder, fully_connected_dim_encoder, fully_connected_dim_decoder,\n",
    "                 target_vocab_size, max_len_enc, max_len_dec,\n",
    "                 enc_output_dim, dropout_rate=0.1, layernorm_eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45608453-8bfe-48fc-88e5-92329c4d6060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (sfe): SpeechFeatureEmbedding(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 1), padding=(0, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=20, out_features=1000, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1000, out_features=20, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout_ffn): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=20, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_emb): TokenEmbedding(\n",
       "      (emb): Embedding(35, 64)\n",
       "      (pos_emb): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dec_layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (mha1): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1000, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1000, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout_ffn): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=35, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(6, 3, 20, 64, #10, 8, , 64\n",
    "                10, 8, 1000, 1000,\n",
    "                35, 234, 399, # 116\n",
    "                (399, 64))  # (, 64)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1754d2fb-6d7b-4124-bb61-18d3401779f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─SpeechFeatureEmbedding: 1-1                 --\n",
      "|    └─Sequential: 2-1                        --\n",
      "|    |    └─Conv2d: 3-1                       640\n",
      "|    |    └─BatchNorm2d: 3-2                  128\n",
      "|    |    └─LeakyReLU: 3-3                    --\n",
      "|    └─Sequential: 2-2                        --\n",
      "|    |    └─Conv2d: 3-4                       65\n",
      "|    |    └─BatchNorm2d: 3-5                  2\n",
      "|    |    └─LeakyReLU: 3-6                    --\n",
      "├─Encoder: 1-2                                --\n",
      "|    └─PositionalEncoding: 2-3                --\n",
      "|    |    └─Dropout: 3-7                      --\n",
      "|    └─ModuleList: 2-4                        --\n",
      "|    |    └─EncoderLayer: 3-8                 42,780\n",
      "|    |    └─EncoderLayer: 3-9                 42,780\n",
      "|    |    └─EncoderLayer: 3-10                42,780\n",
      "|    |    └─EncoderLayer: 3-11                42,780\n",
      "|    |    └─EncoderLayer: 3-12                42,780\n",
      "|    |    └─EncoderLayer: 3-13                42,780\n",
      "|    └─Linear: 2-5                            1,344\n",
      "├─Decoder: 1-3                                --\n",
      "|    └─TokenEmbedding: 2-6                    --\n",
      "|    |    └─Embedding: 3-14                   2,240\n",
      "|    |    └─PositionalEncoding: 3-15          --\n",
      "|    └─ModuleList: 2-7                        --\n",
      "|    |    └─DecoderLayer: 3-16                162,728\n",
      "|    |    └─DecoderLayer: 3-17                162,728\n",
      "|    |    └─DecoderLayer: 3-18                162,728\n",
      "|    └─Dropout: 2-8                           --\n",
      "├─Sequential: 1-4                             --\n",
      "|    └─Linear: 2-9                            2,275\n",
      "======================================================================\n",
      "Total params: 751,558\n",
      "Trainable params: 751,558\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "summary(model)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3347f3-e9d6-4fc8-ade6-9a49518a0f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "train = 9.633496522903442\n",
      "train + load = 20.224180459976196\n",
      "LOSS train 1.1432193517684937\n",
      "Time per epoch 20.22818112373352s\n",
      "EPOCH 2:\n",
      "train = 9.752190589904785\n",
      "train + load = 19.784059047698975\n",
      "LOSS train 1.1434946060180664\n",
      "Time per epoch 19.81504988670349s\n",
      "EPOCH 3:\n",
      "train = 9.695143699645996\n",
      "train + load = 19.738160133361816\n",
      "LOSS train 1.1369510889053345\n",
      "Time per epoch 19.767164707183838s\n",
      "EPOCH 4:\n",
      "train = 9.755335330963135\n",
      "train + load = 19.626372575759888\n",
      "LOSS train 1.135614037513733\n",
      "Time per epoch 19.657378673553467s\n",
      "EPOCH 5:\n",
      "train = 9.810707569122314\n",
      "train + load = 19.906028747558594\n",
      "LOSS train 1.1283148527145386\n",
      "Time per epoch 19.965040922164917s\n",
      "EPOCH 6:\n",
      "train = 9.76753830909729\n",
      "train + load = 20.452537775039673\n",
      "LOSS train 1.12235426902771\n",
      "Time per epoch 20.472543001174927s\n",
      "EPOCH 7:\n",
      "train = 9.60829496383667\n",
      "train + load = 19.260133981704712\n",
      "LOSS train 1.1214927434921265\n",
      "Time per epoch 19.2811381816864s\n",
      "EPOCH 8:\n",
      "train = 9.72814130783081\n",
      "train + load = 19.135021448135376\n",
      "LOSS train 1.1141011714935303\n",
      "Time per epoch 19.155025482177734s\n",
      "EPOCH 9:\n",
      "train = 9.734391927719116\n",
      "train + load = 19.11750555038452\n",
      "LOSS train 1.112230896949768\n",
      "Time per epoch 19.137510299682617s\n",
      "EPOCH 10:\n",
      "train = 9.795456647872925\n",
      "train + load = 19.250014305114746\n",
      "LOSS train 1.1073683500289917\n",
      "Time per epoch 19.28802227973938s\n",
      "EPOCH 11:\n",
      "train = 9.65680193901062\n",
      "train + load = 19.433573484420776\n",
      "LOSS train 1.1038326025009155\n",
      "Time per epoch 19.45957899093628s\n",
      "EPOCH 12:\n",
      "train = 9.784308433532715\n",
      "train + load = 19.403061628341675\n",
      "LOSS train 1.0966942310333252\n",
      "Time per epoch 19.42806649208069s\n",
      "EPOCH 13:\n",
      "train = 9.660740613937378\n",
      "train + load = 19.165440559387207\n",
      "LOSS train 1.092089295387268\n",
      "Time per epoch 19.191445350646973s\n",
      "EPOCH 14:\n",
      "train = 9.75253438949585\n",
      "train + load = 19.25188684463501\n",
      "LOSS train 1.0889182090759277\n",
      "Time per epoch 19.278892517089844s\n",
      "EPOCH 15:\n",
      "train = 9.64315152168274\n",
      "train + load = 19.2602059841156\n",
      "LOSS train 1.0900229215621948\n",
      "Time per epoch 19.304214477539062s\n",
      "EPOCH 16:\n"
     ]
    }
   ],
   "source": [
    "train(model, loss_fn, optimizer, dataloader, epochs=100, model_name=model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "971acd6e-64bb-4ee0-9078-8c188bd0f868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1482345-fbad-4fc4-bd23-456e2ac161bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b574b928-6378-4d11-a86e-4e356358d634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mtensor(vectorizer(\u001b[43minp\u001b[49m))\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inp' is not defined"
     ]
    }
   ],
   "source": [
    "torch.tensor(vectorizer(inp)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b04e5fbc-d9bb-4444-b694-5b4512a2e120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data_mfcc(np_wav, nperseg=1024, samplerate=24000):\n",
    "    mfcc = librosa.feature.mfcc(y=np_wav.astype(float), sr=samplerate, hop_length=nperseg)\n",
    "    pd = sklearn.preprocessing.scale(mfcc, axis=1)\n",
    "    mean = np.mean(pd, axis=1, keepdims=True)\n",
    "    std = np.std(pd, axis=1, keepdims=True)\n",
    "    pd = (pd - mean) / std\n",
    "    new_shape = int(469 * 1024/nperseg)\n",
    "    pad = np.pad(pd, ((0, 0), (0, new_shape - pd.shape[1])), mode='constant')\n",
    "    return torch.tensor(np.expand_dims(np.swapaxes(pad,0,1), axis=0), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f1d4b8d-3b0e-4780-a7e5-648c0f5ff2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = \"<\"\n",
    "for i in range(400 - len(inp)):\n",
    "    res = model(process_data_mfcc(np_wav[0]).unsqueeze(0).to(device), \n",
    "            torch.tensor(vectorizer(inp))[:-1].unsqueeze(0).to(device)).squeeze(0)\n",
    "    #print(len(inp))\n",
    "    inp += voc[res[i].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc6ce01c-f0e8-4578-8fe7-0ca9458af707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<ii  sshuoluda rhea nloewd  thoiunsd etv ehne dwoa sbperreedd  tthoeurgsh  tthhee  wwahsi nwoaukledd  tthhee  wwoorkke daayyss  wfiotuhr etnhde rhea dhiesd  hhiamd  bbeelllllaahh  hhaadd  sshhea rsuhs tthhee  ghoaovke  tthhee  goroeo  sseeeellleeer  dt hgeeeeeeeeeenf  tt  oonneevveerr  tthhee  oofuesrhee  ooutg.e>  oounsee eteh eoe roef  oofuegxheerr  oofu soef  tthhee  oouugghheet htehxec ooufg h'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4347a8a-88f8-409b-a0a5-34c9cd667459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4613d22-e584-4d80-b98a-586845078a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matthew Cuthbert is surprised'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f5f13-f766-44a5-9a76-11115cfe5a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0d992-3c0a-49d1-aa9d-f2ec1d8681dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"0\" * 399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720efd7-9b73-47e7-bde4-bc76e1ab0d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd25f24-df2e-4080-961c-1dfb5356ff99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp2 = \"<\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba293db-64fd-4d61-a1c7-8899a2182476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30a64dfc-07d2-4475-8c0a-e42bf847a358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = model(process_data_mfcc(np_wav[1]).unsqueeze(0).to(device), \n",
    "            torch.tensor(vect_str_list[1])[:-1].unsqueeze(0).to(device)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf7240-e4e1-44cb-a16a-2adbd889dd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3f42127-951f-476b-ae51-bc808eba0c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_str = str()\n",
    "voc = vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc56e615-5ee5-4f71-8f28-65411c224168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = \"<\"\n",
    "for i in range(399):\n",
    "    output_str += voc[res[i].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02e1d70b-9412-4a61-94a6-0ef1f8528db4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i  toct  he sad lioked at thenty aery sush antoallook d tt thxty. aacking t little tf the grornes..>ebbhooooooooooooooooooooooooellleelllleeeeelleeellleeeeleeeeellllllllleeeeeeeaaaaaeeaaaaaeeeeeeeeeeeeeeeeeeeeaaaaaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeellheeee   e hllse   lleee  eeeeeeeaasssssssllssssshlssssassssshhssssaaassaaalsssaasssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssse'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42253f3d-0c90-4338-aef5-2d9962c0660f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In fact, he had looked at twenty very much as he looked at sixty, lacking a little of the grayness.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fe2e9-da00-4d29-af9d-3daa96dd674a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_data_mfcc(np_wav[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02c289-bcba-4043-be53-a154c9881f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = torch.nonzero(torch.eq(tensor, 0))[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48baabcf-e896-43e0-9529-eee03154af5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdcca07-9f07-4cae-8111-d0e04bcc5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.empty(32, 400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226acd3-73ad-465e-a18b-b0e79f649acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tensor.view(-1, tensor.size(-1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc94c67-faf7-4025-898f-bbd4cd605b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf2021-ba53-4d86-b9f0-9b614b0ac369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabd59b-a717-4515-990b-cf2284509ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = process_data_mfcc(np_wav[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72665a-5e82-49e7-81ba-b1aaa5311c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa5f75-e3e3-4836-a529-7835920a8751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(sequence_length): # + batch size * num heads\n",
    "\n",
    "    mask = torch.tril(torch.ones((sequence_length, sequence_length)))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24572524-81c1-447b-b31e-4e55f16ead43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def func(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.masked_fill(mask == 0, int(1)).masked_fill(mask == 1, int(0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d00f4-36e6-4be0-b54a-90c31aeeb637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "func(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d51d1d-0510-4cd6-b0dc-ce1d463b8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_t = nn.MultiheadAttention(embed_dim=2, num_heads=1, dropout=0, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a9ae0-6731-443c-ba40-ca41ae1d115a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[[2, 3],[4, 5],[6, 7]]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e57f7c-490a-4def-9fff-5257da5006d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38eb4b-b6b2-46bd-92bc-6e2667ebb2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o1, o2 = mha_t(tensor, tensor, tensor, attn_mask=func(3), average_attn_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4e8c9-22dc-48c0-87d1-ac21e6a452ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039e93e-f344-4c5c-9822-5c2538ff0c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa151b-4ccf-485b-918f-812f178f6d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn2 = torch.nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454dec94-0627-4861-9335-a1ab2aaa89ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec1 = torch.tensor([1, 2])\n",
    "vec2 = torch.tensor([[0.4 , 0.3, 0.3], [0.2, 0.3, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c55730-742a-4595-ba07-24a0a571fae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4a855-8687-408a-b8ef-71a4d8ffb3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn2(vec2, vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68869e-158f-479b-985f-9f6484324799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6bb05-8587-4d74-bbb6-aea6634ab256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "\n",
    "    features, labels = data\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c708af-9e89-44d8-8ea3-a6efb6124bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model(features, labels[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88674094-5dbd-4a43-9c26-9615b6a7c92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out[0, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0756d-ac58-458b-9b0c-82d425555cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = loss_fn2(out.permute(0, 2, 1), labels[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fca79-7b6b-4ade-9a91-85784fe3e195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b981d-8156-4738-9513-8ddc1945ceb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels[:, 1:].contiguous().view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb38267-e4a2-41bf-90e9-7fda7557636b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.view(-1, out.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abec99-1673-4432-aaba-56fa3c6f1d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nonzero_indices = torch.nonzero(labels[:, 1:].contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c851573-4388-4018-9966-f2851332609e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = labels[:, 1:].contiguous().view(-1)[nonzero_indices].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e88f3f-438b-44f8-bdfc-4f8d0f0a8f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2 = out.view(-1, out.size(-1))[nonzero_indices][:].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25e0fc-990f-4534-8431-4ed6c3930353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8509cf87-76ae-408e-97df-41087404a418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3980d-dbdd-4a55-b9e6-e578dff9a977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn2(t2, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d24107-172d-4389-b7f7-adce6dbdbf40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
